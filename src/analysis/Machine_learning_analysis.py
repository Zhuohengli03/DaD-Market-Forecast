import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import VotingRegressor, BaggingRegressor, StackingRegressor
from sklearn.model_selection import KFold
# å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import xgboost as xgb
    from xgboost import XGBRegressor
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("âš ï¸ XGBoost not available, using alternative models")

try:
    from prophet import Prophet
    HAS_PROPHET = True
except ImportError:
    HAS_PROPHET = False

try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    HAS_TENSORFLOW = True
except ImportError:
    HAS_TENSORFLOW = False

try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import seasonal_decompose
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MarketMLAnalyzer:
    def __init__(self, csv_file_path):
        """
        åˆå§‹åŒ–æœºå™¨å­¦ä¹ åˆ†æå™¨
        
        Args:
            csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        """
        self.csv_file_path = csv_file_path
        self.df = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.models = {}
        self.best_model = None
        self.feature_importance = None
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ”„ åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®
            self.df = pd.read_csv(self.csv_file_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(self.df)} æ¡è®°å½•")
            
            # æ•°æ®åŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.df['created_at'].min()} åˆ° {self.df['created_at'].max()}")
            
            # æ•°æ®é¢„å¤„ç†
            self._preprocess_data()
            
            # ç‰¹å¾å·¥ç¨‹
            self._feature_engineering()
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            self._prepare_training_data()
            
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return False
            
        return True
    
    def _preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ”§ è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        
        # è½¬æ¢æ—¶é—´åˆ—
        self.df['created_at'] = pd.to_datetime(self.df['created_at'])
        self.df['insert_time'] = pd.to_datetime(self.df['insert_time'])
        
        # æ’åº
        self.df = self.df.sort_values('created_at').reset_index(drop=True)
        
        # å¤„ç†ç¼ºå¤±å€¼
        self.df = self.df.dropna()
        
        # æ•°æ®ç±»å‹è½¬æ¢
        self.df['quantity'] = pd.to_numeric(self.df['quantity'], errors='coerce')
        self.df['price_per_unit'] = pd.to_numeric(self.df['price_per_unit'], errors='coerce')
        self.df['price'] = pd.to_numeric(self.df['price'], errors='coerce')
        self.df['has_sold'] = self.df['has_sold'].astype(int)
        
        # ç§»é™¤å¼‚å¸¸å€¼
        self._remove_outliers()
        
        print(f"ğŸ“Š é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {self.df.shape}")
    
    def _remove_outliers(self):
        """ç§»é™¤å¼‚å¸¸å€¼ - ä½¿ç”¨æ”¹è¿›çš„Z-scoreæ–¹æ³•"""
        print("ğŸ” æ£€æµ‹å’Œç§»é™¤å¼‚å¸¸å€¼...")
        
        # ä½¿ç”¨Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼Œå¯¹ä»·æ ¼æ•°æ®ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
        numeric_columns = ['quantity', 'price_per_unit', 'price']
        original_shape = self.df.shape
        
        for col in numeric_columns:
            # è®¡ç®—Z-score
            z_scores = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())
            
            # å¯¹ä»·æ ¼æ•°æ®ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼ˆ2.5Ïƒï¼‰ï¼Œå¯¹æ•°é‡ä½¿ç”¨æ ‡å‡†é˜ˆå€¼ï¼ˆ3Ïƒï¼‰
            threshold = 2.5 if col in ['price_per_unit', 'price'] else 3.0
            outliers = z_scores > threshold
            outlier_count = outliers.sum()
            
            if outlier_count > 0:
                print(f"  {col}: å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼ (é˜ˆå€¼: {threshold}Ïƒ)")
                self.df = self.df[~outliers]
        
        removed_count = original_shape[0] - self.df.shape[0]
        print(f"ğŸ“Š ç§»é™¤å¼‚å¸¸å€¼åæ•°æ®å½¢çŠ¶: {self.df.shape} (ç§»é™¤äº† {removed_count} æ¡è®°å½•)")
    
    def _feature_engineering(self):
        """æ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹ - é¿å…æ•°æ®æ³„éœ²"""
        print("âš™ï¸ è¿›è¡Œæ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰...")
        
        # æŒ‰æ—¶é—´æ’åºï¼Œç¡®ä¿æ—¶é—´åºåˆ—çš„æ­£ç¡®æ€§
        self.df = self.df.sort_values('created_at').reset_index(drop=True)
        
        # 1. åŸºç¡€æ—¶é—´ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
        self.df['hour'] = self.df['created_at'].dt.hour
        self.df['day_of_week'] = self.df['created_at'].dt.dayofweek
        self.df['day_of_month'] = self.df['created_at'].dt.day
        self.df['month'] = self.df['created_at'].dt.month
        self.df['is_weekend'] = (self.df['day_of_week'] >= 5).astype(int)
        
        # 2. åŸºç¡€æ•°å€¼ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
        self.df['price_quantity_ratio'] = self.df['price'] / (self.df['quantity'] + 1)
        self.df['quantity_squared'] = self.df['quantity'] ** 2
        self.df['price_quantity_interaction'] = self.df['price'] * self.df['quantity']  # ä½¿ç”¨æ€»ä»·ï¼Œä¸æ˜¯å•ä»·
        
        # 3. ä¸¥æ ¼çš„æ»åç‰¹å¾ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
        for lag in [1, 2, 3, 5, 10]:
            self.df[f'price_lag_{lag}'] = self.df['price_per_unit'].shift(lag)
            self.df[f'quantity_lag_{lag}'] = self.df['quantity'].shift(lag)
            
        # 4. åŸºäºæ»åä»·æ ¼çš„ç§»åŠ¨å¹³å‡ï¼ˆå®‰å…¨ï¼‰
        for window in [3, 5, 7]:
            # ä½¿ç”¨å·²ç»æ»åçš„ä»·æ ¼è®¡ç®—ç§»åŠ¨å¹³å‡
            self.df[f'price_lag1_ma_{window}'] = self.df['price_lag_1'].rolling(window=window, min_periods=1).mean()
            self.df[f'quantity_ma_{window}'] = self.df['quantity'].rolling(window=window, min_periods=1).mean()
        
        # 5. åŸºäºæ»åä»·æ ¼çš„æ³¢åŠ¨æ€§ï¼ˆå®‰å…¨ï¼‰
        for window in [5, 10]:
            self.df[f'price_lag1_volatility_{window}'] = self.df['price_lag_1'].rolling(window=window, min_periods=1).std()
            
        # 6. ä»·æ ¼å˜åŒ–ï¼ˆåŸºäºæ»åï¼‰
        self.df['price_lag1_change'] = self.df['price_lag_1'].diff()
        self.df['price_lag1_change_pct'] = self.df['price_lag_1'].pct_change()
        
        # 7. äº¤äº’ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
        self.df['hour_quantity_interaction'] = self.df['hour'] * self.df['quantity']
        self.df['day_quantity_interaction'] = self.df['day_of_week'] * self.df['quantity']
        
        # 8. å¡«å……ç¼ºå¤±å€¼ï¼ˆå‘å‰å¡«å……ï¼Œé¿å…ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼‰
        self.df = self.df.fillna(method='ffill').fillna(0)
        
        print("âœ… æ”¹è¿›çš„ç‰¹å¾å·¥ç¨‹å®Œæˆï¼ˆå·²é¿å…æ•°æ®æ³„éœ²ï¼‰")
    
    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # é€‰æ‹©å®‰å…¨ç‰¹å¾ - ä¸¥æ ¼é¿å…æ•°æ®æ³„éœ²
        feature_columns = [
            # åŸºç¡€ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
            'quantity', 'has_sold',
            # æ—¶é—´ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
            'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend',
            # åŸºç¡€æ•°å€¼ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
            'price_quantity_ratio', 'quantity_squared', 'price_quantity_interaction',
            # æ»åç‰¹å¾ï¼ˆå®‰å…¨ - ä½¿ç”¨å†å²ä»·æ ¼ï¼‰
            'price_lag_1', 'price_lag_2', 'price_lag_3', 'price_lag_5', 'price_lag_10',
            'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3', 'quantity_lag_5', 'quantity_lag_10',
            # åŸºäºæ»åä»·æ ¼çš„ç§»åŠ¨å¹³å‡ï¼ˆå®‰å…¨ï¼‰
            'price_lag1_ma_3', 'price_lag1_ma_5', 'price_lag1_ma_7',
            'quantity_ma_3', 'quantity_ma_5', 'quantity_ma_7',
            # åŸºäºæ»åä»·æ ¼çš„æ³¢åŠ¨æ€§ï¼ˆå®‰å…¨ï¼‰
            'price_lag1_volatility_5', 'price_lag1_volatility_10',
            # åŸºäºæ»åä»·æ ¼çš„å˜åŒ–ï¼ˆå®‰å…¨ï¼‰
            'price_lag1_change', 'price_lag1_change_pct',
            # äº¤äº’ç‰¹å¾ï¼ˆå®‰å…¨ï¼‰
            'hour_quantity_interaction', 'day_quantity_interaction'
        ]
        
        # ç§»é™¤åŒ…å«NaNçš„åˆ—
        available_features = [col for col in feature_columns if col in self.df.columns]
        X_temp = self.df[available_features].fillna(0)
        self.y = self.df['price_per_unit']
        
        print(f"ğŸ“Š åˆå§‹ç‰¹å¾æ•°é‡: {len(available_features)}")
        
        # ç‰¹å¾é€‰æ‹© - å‡å°‘è¿‡æ‹Ÿåˆé£é™©
        if len(available_features) > 15:  # åªæœ‰åœ¨ç‰¹å¾è¿‡å¤šæ—¶æ‰è¿›è¡Œé€‰æ‹©
            print("ğŸ” è¿›è¡Œç‰¹å¾é€‰æ‹©ä»¥å‡å°‘è¿‡æ‹Ÿåˆ...")
            
            # ä½¿ç”¨SelectKBesté€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
            k_best = min(15, len(available_features))  # æœ€å¤šé€‰æ‹©15ä¸ªç‰¹å¾
            selector = SelectKBest(score_func=f_regression, k=k_best)
            
            # æš‚æ—¶åˆ†å‰²æ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©
            temp_split = int(len(X_temp) * 0.8)
            X_train_temp = X_temp.iloc[:temp_split]
            y_train_temp = self.y.iloc[:temp_split]
            
            X_selected = selector.fit_transform(X_train_temp, y_train_temp)
            selected_features = [available_features[i] for i in selector.get_support(indices=True)]
            
            print(f"ğŸ“Š é€‰æ‹©çš„ç‰¹å¾: {len(selected_features)} ä¸ª")
            print(f"   ç‰¹å¾åˆ—è¡¨: {selected_features}")
            
            self.X = self.df[selected_features].fillna(0)
        else:
            print("ğŸ“Š ç‰¹å¾æ•°é‡é€‚ä¸­ï¼Œæ— éœ€ç‰¹å¾é€‰æ‹©")
            self.X = X_temp
        
        print(f"ğŸ“Š æœ€ç»ˆç‰¹å¾æ•°é‡: {len(self.X.columns)}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(self.X)}")
        
        # æ—¶é—´åºåˆ—åˆ†å‰² - ç¡®ä¿è®­ç»ƒé›†åœ¨æµ‹è¯•é›†ä¹‹å‰
        split_index = int(len(self.X) * 0.8)
        
        # ç¡®ä¿åˆ†å‰²ç‚¹å›ºå®šï¼Œé¿å…éšæœºæ€§
        self.X_train = self.X.iloc[:split_index].copy()
        self.X_test = self.X.iloc[split_index:].copy()
        self.y_train = self.y.iloc[:split_index].copy()
        self.y_test = self.y.iloc[split_index:].copy()
        
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(self.X_train)}, æµ‹è¯•é›†å¤§å°: {len(self.X_test)}")
        print(f"ğŸ“Š åˆ†å‰²ç‚¹: ç¬¬{split_index}æ¡è®°å½• ({(split_index/len(self.X)*100):.1f}%)")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print("âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
    
    def train_models(self):
        """è®­ç»ƒå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # å®šä¹‰ä¼˜åŒ–æ¨¡å‹ - å¢å¼ºæ€§èƒ½å’Œç¨³å®šæ€§
        models = {
            # é›†æˆå­¦ä¹ æ¨¡å‹
            'Random Forest': RandomForestRegressor(
                n_estimators=200, max_depth=12, min_samples_split=5, 
                min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1
            ),
            'Extra Trees': ExtraTreesRegressor(
                n_estimators=200, max_depth=12, min_samples_split=5,
                min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.03, 
                subsample=0.8, random_state=42
            ),
            # çº¿æ€§æ¨¡å‹
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=0.1, random_state=42),
            'Lasso Regression': Lasso(alpha=0.01, random_state=42, max_iter=5000),
            'Elastic Net': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=5000),
            # æ”¯æŒå‘é‡æœº
            'SVR': SVR(kernel='rbf', C=10.0, gamma='auto', epsilon=0.01),
            # ç¥ç»ç½‘ç»œ
            'MLP Regressor': MLPRegressor(
                hidden_layer_sizes=(100, 50), activation='relu', solver='adam',
                alpha=0.001, learning_rate='adaptive', max_iter=1000, random_state=42
            )
        }
        
        # æ·»åŠ  XGBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if HAS_XGBOOST:
            models['XGBoost'] = XGBRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.03,
                subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
            )
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        model_scores = {}
        
        for name, model in models.items():
            print(f"ğŸ”„ è®­ç»ƒ {name}...")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                if name == 'SVR':
                    model.fit(self.X_train_scaled, self.y_train)
                    y_pred = model.predict(self.X_test_scaled)
                else:
                    model.fit(self.X_train, self.y_train)
                    y_pred = model.predict(self.X_test)
                
                # è¯„ä¼°æ¨¡å‹
                mae = mean_absolute_error(self.y_test, y_pred)
                mse = mean_squared_error(self.y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(self.y_test, y_pred)
                
                # æ·»åŠ äº¤å‰éªŒè¯è¯„ä¼°
                if name == 'SVR':
                    cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=3, scoring='r2')
                else:
                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3, scoring='r2')
                
                model_scores[name] = {
                    'model': model,
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'cv_r2_mean': cv_scores.mean(),
                    'cv_r2_std': cv_scores.std(),
                    'predictions': y_pred
                }
                
                print(f"  âœ… {name}: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.3f}, CV-RÂ²={cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
                
            except Exception as e:
                print(f"  âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
        
        self.models = model_scores
        
        # åˆ›å»ºå¤šå±‚æ¬¡é›†æˆæ¨¡å‹
        print("\nğŸ”„ åˆ›å»ºå¤šå±‚æ¬¡é›†æˆæ¨¡å‹...")
        ensemble_models = self._create_ensemble_model(model_scores)
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        if ensemble_models:
            ensemble_scores = self._evaluate_ensemble_model(ensemble_models)
            if ensemble_scores:
                model_scores['Ensemble'] = ensemble_scores
                print(f"âœ… æœ€ä½³é›†æˆæ¨¡å‹: RÂ²={ensemble_scores['r2']:.3f}, CV-RÂ²={ensemble_scores['cv_r2_mean']:.3f}")
                
                # æ‰“å°é›†æˆç»Ÿè®¡ä¿¡æ¯
                diversity = ensemble_models.get('model_diversity', 0)
                print(f"ğŸ”„ æ¨¡å‹å¤šæ ·æ€§: {diversity} ç§ä¸åŒç±»å‹")
            else:
                print("âš ï¸ é›†æˆæ¨¡å‹åˆ›å»ºå¤±è´¥")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ - ä½¿ç”¨äº¤å‰éªŒè¯åˆ†æ•°
        best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['cv_r2_mean'])
        self.best_model = model_scores[best_model_name]['model']
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"ğŸ“Š æœ€ä½³æ¨¡å‹æ€§èƒ½: RÂ²={model_scores[best_model_name]['r2']:.3f}, CV-RÂ²={model_scores[best_model_name]['cv_r2_mean']:.3f}")
        
        # è¯¦ç»†æ¨¡å‹å‡†ç¡®æ€§åˆ†æ
        self._analyze_model_accuracy(model_scores[best_model_name], best_model_name)
        
        return model_scores
    
    def _create_ensemble_model(self, model_scores):
        """åˆ›å»ºå¤šå±‚æ¬¡é›†æˆæ¨¡å‹ - å¢å¼ºç¨³å®šæ€§å’Œå‡†ç¡®æ€§"""
        try:
            # é€‰æ‹©æ€§èƒ½æœ€ä½³çš„æ¨¡å‹
            top_models = sorted(model_scores.items(), key=lambda x: x[1]['cv_r2_mean'], reverse=True)
            
            if len(top_models) < 3:
                print("âš ï¸ æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡é›†æˆ")
                return None
                
            # 1. å‡†å¤‡ä¸åŒç±»å‹çš„æ¨¡å‹ç»„åˆ
            linear_models = []
            tree_models = []
            other_models = []
            
            for name, scores in top_models:
                model = scores['model']
                if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Elastic Net']:
                    linear_models.append((name, model))
                elif name in ['Random Forest', 'Extra Trees', 'Gradient Boosting', 'XGBoost']:
                    tree_models.append((name, model))
                else:
                    other_models.append((name, model))
            
            ensemble_results = {}
            
            # ç®€åŒ–é›†æˆç­–ç•¥ - åªä¿ç•™æœ€æœ‰æ•ˆçš„æ–¹æ³•
            
            # 1. Voting Ensemble (æŠ•ç¥¨é›†æˆ) - ä¸»è¦æ–¹æ³•
            if len(top_models) >= 3:
                voting_estimators = [(name, scores['model']) for name, scores in top_models[:3]]  # åªä½¿ç”¨å‰3ä¸ª
                # ä½¿ç”¨åŠ¨æ€æƒé‡ï¼ˆåŸºäºRÂ²å’Œç¨³å®šæ€§ï¼‰
                weights = self._calculate_dynamic_weights(top_models[:3])
                
                voting_ensemble = VotingRegressor(
                    estimators=voting_estimators,
                    weights=weights
                )
                ensemble_results['voting'] = voting_ensemble
            
            # 2. ç®€å•åŠ æƒå¹³å‡ (å¤‡é€‰æ–¹æ³•)
            ensemble_results['weighted_average'] = {
                'models': [scores['model'] for name, scores in top_models[:3]],
                'weights': self._calculate_dynamic_weights(top_models[:3]) if len(top_models) >= 3 else None
            }
            
            return {
                'ensembles': ensemble_results,
                'top_models': top_models,
                'model_diversity': len(set([type(scores['model']).__name__ for name, scores in top_models]))
            }
            
        except Exception as e:
            print(f"âš ï¸ é›†æˆæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _calculate_dynamic_weights(self, top_models):
        """è®¡ç®—åŠ¨æ€æƒé‡ - ç»¼åˆè€ƒè™‘RÂ²ã€ç¨³å®šæ€§å’Œå¤šæ ·æ€§"""
        weights = []
        
        for name, scores in top_models:
            # åŸºç¡€æƒé‡ï¼šRÂ²åˆ†æ•°
            base_weight = scores['cv_r2_mean']
            
            # ç¨³å®šæ€§åŠ åˆ†ï¼šæ ‡å‡†å·®è¶Šå°è¶Šå¥½
            stability_bonus = 1.0 - min(scores['cv_r2_std'], 0.1) * 10
            
            # æ¨¡å‹ç±»å‹å¤šæ ·æ€§åŠ åˆ†
            if name in ['Linear Regression', 'Ridge Regression']:
                diversity_bonus = 1.1  # çº¿æ€§æ¨¡å‹ç¨³å®šæ€§åŠ åˆ†
            elif name in ['Random Forest', 'Gradient Boosting']:
                diversity_bonus = 1.05  # é›†æˆæ¨¡å‹åŠ åˆ†
            else:
                diversity_bonus = 1.0
                
            final_weight = base_weight * stability_bonus * diversity_bonus
            weights.append(final_weight)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights)
        if total_weight > 0:
            weights = [w / total_weight for w in weights]
        else:
            weights = [1.0 / len(weights)] * len(weights)
            
        return weights
    
    def _evaluate_ensemble_model(self, ensemble_models):
        """è¯„ä¼°å¤šç§é›†æˆæ¨¡å‹å¹¶é€‰æ‹©æœ€ä½³çš„"""
        try:
            ensembles = ensemble_models['ensembles']
            ensemble_scores = {}
            
            print(f"\nğŸ”„ è¯„ä¼° {len(ensembles)} ç§é›†æˆç­–ç•¥...")
            
            for ensemble_name, ensemble_model in ensembles.items():
                try:
                    if ensemble_name == 'weighted_average':
                        # åŠ æƒå¹³å‡é›†æˆçš„ç‰¹æ®Šå¤„ç†
                        predictions = self._multi_model_predict(ensemble_model['models'], ensemble_model['weights'])
                        y_pred = predictions
                    else:
                        # æ ‡å‡†é›†æˆæ¨¡å‹
                        ensemble_model.fit(self.X_train, self.y_train)
                        y_pred = ensemble_model.predict(self.X_test)
                    
                    # è¯„ä¼°æŒ‡æ ‡
                    mae = mean_absolute_error(self.y_test, y_pred)
                    mse = mean_squared_error(self.y_test, y_pred)
                    rmse = np.sqrt(mse)
                    r2 = r2_score(self.y_test, y_pred)
                    
                    # äº¤å‰éªŒè¯ï¼ˆé™¤äº†åŠ æƒå¹³å‡ï¼‰
                    if ensemble_name != 'weighted_average':
                        cv_scores = cross_val_score(ensemble_model, self.X_train, self.y_train, cv=3, scoring='r2')
                        cv_r2_mean = cv_scores.mean()
                        cv_r2_std = cv_scores.std()
                    else:
                        # åŠ æƒå¹³å‡çš„äº¤å‰éªŒè¯éœ€è¦ç‰¹æ®Šå¤„ç†
                        cv_r2_mean = r2  # ä½¿ç”¨æµ‹è¯•é›†RÂ²ä½œä¸ºä¼°è®¡
                        cv_r2_std = 0.001  # å‡è®¾è¾ƒå°çš„æ ‡å‡†å·®
                    
                    ensemble_scores[ensemble_name] = {
                        'model': ensemble_model,
                        'mae': mae,
                        'mse': mse,
                        'rmse': rmse,
                        'r2': r2,
                        'cv_r2_mean': cv_r2_mean,
                        'cv_r2_std': cv_r2_std,
                        'predictions': y_pred
                    }
                    
                    print(f"  âœ… {ensemble_name}: RÂ²={r2:.3f}, CV-RÂ²={cv_r2_mean:.3f}Â±{cv_r2_std:.3f}")
                    
                except Exception as e:
                    print(f"  âš ï¸ {ensemble_name} è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue
            
            # é€‰æ‹©æœ€ä½³é›†æˆæ¨¡å‹
            if ensemble_scores:
                best_ensemble_name = max(ensemble_scores.keys(), key=lambda x: ensemble_scores[x]['cv_r2_mean'])
                best_ensemble = ensemble_scores[best_ensemble_name]
                
                print(f"\nğŸ† æœ€ä½³é›†æˆç­–ç•¥: {best_ensemble_name}")
                print(f"ğŸ“Š é›†æˆæ€§èƒ½: RÂ²={best_ensemble['r2']:.3f}, CV-RÂ²={best_ensemble['cv_r2_mean']:.3f}")
                
                return best_ensemble
            else:
                print("âš ï¸ æ‰€æœ‰é›†æˆæ¨¡å‹éƒ½å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âš ï¸ é›†æˆæ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def _multi_model_predict(self, models, weights):
        """å¤šæ¨¡å‹åŠ æƒå¹³å‡é¢„æµ‹"""
        if weights is None:
            weights = [1.0 / len(models)] * len(models)
            
        predictions = np.zeros(len(self.X_test))
        
        for i, model in enumerate(models):
            try:
                # ç¡®ä¿æ¨¡å‹å·²è®­ç»ƒ
                if not hasattr(model, 'predict') or not hasattr(model, 'coef_') and not hasattr(model, 'feature_importances_') and not hasattr(model, 'support_vectors_'):
                    model.fit(self.X_train, self.y_train)
                
                pred = model.predict(self.X_test)
                predictions += weights[i] * pred
            except Exception as e:
                print(f"  âš ï¸ æ¨¡å‹ {i} é¢„æµ‹å¤±è´¥: {str(e)}")
                continue
                
        return predictions
    
    def _analyze_model_accuracy(self, model_info, model_name):
        """è¯¦ç»†åˆ†ææ¨¡å‹å‡†ç¡®æ€§"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹å‡†ç¡®æ€§è¯¦ç»†åˆ†æ")
        print("="*60)
        
        # 1. åŸºç¡€æŒ‡æ ‡åˆ†æ
        print("\nğŸ” åŸºç¡€è¯„ä¼°æŒ‡æ ‡:")
        print(f"  â€¢ RÂ² å†³å®šç³»æ•°: {model_info['r2']:.4f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
        print(f"  â€¢ äº¤å‰éªŒè¯ RÂ²: {model_info['cv_r2_mean']:.4f} Â± {model_info['cv_r2_std']:.4f}")
        print(f"  â€¢ å¹³å‡ç»å¯¹è¯¯å·® (MAE): {model_info['mae']:.4f}")
        print(f"  â€¢ å‡æ–¹æ ¹è¯¯å·® (RMSE): {model_info['rmse']:.4f}")
        
        # 2. å‡†ç¡®æ€§ç­‰çº§è¯„ä¼°
        r2_score = model_info['r2']
        cv_r2_score = model_info['cv_r2_mean']
        
        print("\nğŸ“ˆ å‡†ç¡®æ€§ç­‰çº§è¯„ä¼°:")
        if r2_score >= 0.95:
            print("  âœ… ä¼˜ç§€ (RÂ² â‰¥ 0.95) - æ¨¡å‹é¢„æµ‹éå¸¸å‡†ç¡®")
        elif r2_score >= 0.85:
            print("  âœ… è‰¯å¥½ (0.85 â‰¤ RÂ² < 0.95) - æ¨¡å‹é¢„æµ‹è¾ƒä¸ºå‡†ç¡®")
        elif r2_score >= 0.70:
            print("  âš ï¸  ä¸€èˆ¬ (0.70 â‰¤ RÂ² < 0.85) - æ¨¡å‹é¢„æµ‹åŸºæœ¬å¯ç”¨")
        elif r2_score >= 0.50:
            print("  âš ï¸  è¾ƒå·® (0.50 â‰¤ RÂ² < 0.70) - æ¨¡å‹é¢„æµ‹ä¸å¤Ÿå‡†ç¡®")
        else:
            print("  âŒ å¾ˆå·® (RÂ² < 0.50) - æ¨¡å‹é¢„æµ‹ä¸å‡†ç¡®")
        
        # 3. è¿‡æ‹Ÿåˆæ£€æµ‹
        print("\nğŸ” è¿‡æ‹Ÿåˆæ£€æµ‹:")
        r2_diff = r2_score - cv_r2_score
        if r2_diff > 0.1:
            print(f"  âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        elif r2_diff > 0.05:
            print(f"  âš ï¸  è½»å¾®è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        else:
            print(f"  âœ… æ— è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        
        # 4. é¢„æµ‹è¯¯å·®åˆ†æ
        y_pred = model_info['predictions']
        errors = self.y_test - y_pred
        
        print("\nğŸ“Š é¢„æµ‹è¯¯å·®åˆ†æ:")
        print(f"  â€¢ å¹³å‡è¯¯å·®: {errors.mean():.4f} (è¶Šæ¥è¿‘0è¶Šå¥½)")
        print(f"  â€¢ è¯¯å·®æ ‡å‡†å·®: {errors.std():.4f}")
        print(f"  â€¢ æœ€å¤§æ­£è¯¯å·®: {errors.max():.4f}")
        print(f"  â€¢ æœ€å¤§è´Ÿè¯¯å·®: {errors.min():.4f}")
        
        # 5. ç›¸å¯¹è¯¯å·®åˆ†æ
        relative_errors = np.abs(errors) / self.y_test * 100
        print(f"  â€¢ å¹³å‡ç›¸å¯¹è¯¯å·®: {relative_errors.mean():.2f}% (è¶Šæ¥è¿‘0è¶Šå¥½)")
        print(f"  â€¢ ç›¸å¯¹è¯¯å·®ä¸­ä½æ•°: {relative_errors.median():.2f}%")
        
        # 6. é¢„æµ‹èŒƒå›´åˆ†æ
        print("\nğŸ“ˆ é¢„æµ‹èŒƒå›´åˆ†æ:")
        print(f"  â€¢ å®é™…ä»·æ ¼èŒƒå›´: {self.y_test.min():.2f} - {self.y_test.max():.2f}")
        print(f"  â€¢ é¢„æµ‹ä»·æ ¼èŒƒå›´: {y_pred.min():.2f} - {y_pred.max():.2f}")
        print(f"  â€¢ ä»·æ ¼èŒƒå›´è¦†ç›–ç‡: {((y_pred >= self.y_test.min()) & (y_pred <= self.y_test.max())).mean()*100:.1f}%")
        
        # 7. æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°
        print("\nğŸ›¡ï¸ æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°:")
        cv_std = model_info['cv_r2_std']
        if cv_std < 0.01:
            print("  âœ… éå¸¸ç¨³å®š (CVæ ‡å‡†å·® < 0.01)")
        elif cv_std < 0.05:
            print("  âœ… ç¨³å®š (CVæ ‡å‡†å·® < 0.05)")
        elif cv_std < 0.10:
            print("  âš ï¸  ä¸€èˆ¬ç¨³å®š (CVæ ‡å‡†å·® < 0.10)")
        else:
            print("  âŒ ä¸ç¨³å®š (CVæ ‡å‡†å·® â‰¥ 0.10)")
        
        # 8. ä¸šåŠ¡ä»·å€¼è¯„ä¼°
        print("\nğŸ’¼ ä¸šåŠ¡ä»·å€¼è¯„ä¼°:")
        mae_percentage = (model_info['mae'] / self.y_test.mean()) * 100
        if mae_percentage < 5:
            print("  âœ… é«˜ä»·å€¼ - é¢„æµ‹è¯¯å·®å°äº5%ï¼Œé€‚åˆæŠ•èµ„å†³ç­–")
        elif mae_percentage < 10:
            print("  âœ… ä¸­é«˜ä»·å€¼ - é¢„æµ‹è¯¯å·®5-10%ï¼Œé€‚åˆè¶‹åŠ¿åˆ†æ")
        elif mae_percentage < 20:
            print("  âš ï¸  ä¸­ç­‰ä»·å€¼ - é¢„æµ‹è¯¯å·®10-20%ï¼Œé€‚åˆå‚è€ƒ")
        else:
            print("  âŒ ä½ä»·å€¼ - é¢„æµ‹è¯¯å·®å¤§äº20%ï¼Œä¸å»ºè®®ç”¨äºå†³ç­–")
        
        print("\n" + "="*60)
    
    def hyperparameter_tuning(self):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("âš™ï¸ å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ ¹æ®æœ€ä½³æ¨¡å‹ç±»å‹è¿›è¡Œè°ƒä¼˜
        if isinstance(self.best_model, RandomForestRegressor):
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            }
            model = RandomForestRegressor(random_state=42)
            
        elif isinstance(self.best_model, GradientBoostingRegressor):
            param_grid = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            }
            model = GradientBoostingRegressor(random_state=42)
            
        else:
            print("âš ï¸ å½“å‰æœ€ä½³æ¨¡å‹ä¸æ”¯æŒè¶…å‚æ•°è°ƒä¼˜")
            return self.best_model
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            model, param_grid, cv=5, scoring='r2', n_jobs=-1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"ğŸ“Š æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.3f}")
        
        self.best_model = grid_search.best_estimator_
        return self.best_model
    
    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print("ğŸ” åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance = pd.DataFrame({
                'feature': self.X.columns,
                'importance': self.best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
            print(self.feature_importance.head(10))
            
        else:
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        
        return self.feature_importance
    
    def test_model_stability(self, n_runs=5):
        """æµ‹è¯•æ¨¡å‹ç¨³å®šæ€§"""
        print(f"\nğŸ”„ æµ‹è¯•æ¨¡å‹ç¨³å®šæ€§ ({n_runs} æ¬¡è¿è¡Œ)...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é›†æˆæ¨¡å‹
        if isinstance(self.best_model, dict):
            print("ğŸ”„ æ£€æµ‹åˆ°é›†æˆæ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹è¿›è¡Œç¨³å®šæ€§æµ‹è¯•...")
            # æ‰¾åˆ°æœ€ä½³çš„éé›†æˆæ¨¡å‹
            single_models = {k: v for k, v in self.models.items() if k != 'Ensemble'}
            if not single_models:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„å•ä¸€æ¨¡å‹è¿›è¡Œç¨³å®šæ€§æµ‹è¯•")
                return None
            best_single_name = max(single_models.keys(), key=lambda x: single_models[x]['cv_r2_mean'])
            stability_model = single_models[best_single_name]['model']
            print(f"  ä½¿ç”¨ {best_single_name} è¿›è¡Œç¨³å®šæ€§æµ‹è¯•")
        else:
            stability_model = self.best_model
        
        # å­˜å‚¨å¤šæ¬¡è¿è¡Œçš„ç»“æœ
        r2_scores = []
        mae_scores = []
        predictions_list = []
        
        for i in range(n_runs):
            print(f"  è¿è¡Œ {i+1}/{n_runs}...")
            
            try:
                # åˆ›å»ºæ¨¡å‹çš„æ–°å®ä¾‹ï¼ˆé¿å…ä¿®æ”¹åŸæ¨¡å‹ï¼‰
                from sklearn.base import clone
                test_model = clone(stability_model)
                
                # è®¾ç½®éšæœºç§å­ä»¥è·å¾—ä¸åŒçš„ç»“æœ
                if hasattr(test_model, 'random_state'):
                    test_model.random_state = 42 + i
                
                # è®­ç»ƒæ¨¡å‹
                if 'SVR' in str(type(test_model)):
                    test_model.fit(self.X_train_scaled, self.y_train)
                    y_pred = test_model.predict(self.X_test_scaled)
                else:
                    test_model.fit(self.X_train, self.y_train)
                    y_pred = test_model.predict(self.X_test)
                
                # è¯„ä¼°
                r2 = r2_score(self.y_test, y_pred)
                mae = mean_absolute_error(self.y_test, y_pred)
                
                r2_scores.append(r2)
                mae_scores.append(mae)
                predictions_list.append(y_pred)
                
            except Exception as e:
                print(f"  âš ï¸ è¿è¡Œ {i+1} å¤±è´¥: {str(e)}")
                continue
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        r2_mean = np.mean(r2_scores)
        r2_std = np.std(r2_scores)
        mae_mean = np.mean(mae_scores)
        mae_std = np.std(mae_scores)
        
        print(f"\nğŸ“Š ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
        print(f"  RÂ² å¹³å‡å€¼: {r2_mean:.4f} Â± {r2_std:.4f}")
        print(f"  MAE å¹³å‡å€¼: {mae_mean:.4f} Â± {mae_std:.4f}")
        
        # ç¨³å®šæ€§è¯„ä¼°
        if r2_std < 0.01:
            print("  âœ… éå¸¸ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.01)")
        elif r2_std < 0.05:
            print("  âœ… ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.05)")
        elif r2_std < 0.10:
            print("  âš ï¸  ä¸€èˆ¬ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.10)")
        else:
            print("  âŒ ä¸ç¨³å®š (RÂ²æ ‡å‡†å·® â‰¥ 0.10)")
        
        return {
            'r2_scores': r2_scores,
            'mae_scores': mae_scores,
            'r2_mean': r2_mean,
            'r2_std': r2_std,
            'mae_mean': mae_mean,
            'mae_std': mae_std,
            'predictions': predictions_list
        }
    
    def predict_future_prices(self, days_ahead=7, method='auto'):
        """é¢„æµ‹æœªæ¥ä»·æ ¼ - ä½¿ç”¨æ”¹è¿›çš„é¢„æµ‹æ–¹æ³•"""
        print(f"ğŸ”® é¢„æµ‹æœªæ¥ {days_ahead} å¤©çš„ä»·æ ¼ (æ–¹æ³•: {method})...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # ä½¿ç”¨å†…ç½®é¢„æµ‹æ–¹æ³•
        predictions, confidence_intervals = self._predict_with_best_model(days_ahead, method)
        
        # ç”Ÿæˆç®€åŒ–çš„æ¨¡å‹è§£é‡ŠæŠ¥å‘Š
        if method == 'auto' or 'interpret' in str(method):
            print("\nğŸ§  ç”Ÿæˆæ¨¡å‹è§£é‡ŠæŠ¥å‘Š...")
            self._generate_simple_interpretation()
        
        # åˆ›å»ºé¢„æµ‹ç»“æœ
        future_dates = pd.date_range(
            start=self.df['created_at'].max() + pd.Timedelta(days=1),
            periods=days_ahead,
            freq='D'
        )
        
        predictions_df = pd.DataFrame({
            'date': future_dates,
            'predicted_price': predictions,
            'ci_lower': [ci[0] for ci in confidence_intervals],
            'ci_upper': [ci[1] for ci in confidence_intervals]
        })
        
        print("âœ… ä»·æ ¼é¢„æµ‹å®Œæˆ")
        return predictions_df
    
    def _predict_with_best_model(self, days_ahead, method='auto'):
        """ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œæ”¯æŒå¤šæ–¹æ³•èåˆ"""
        if method == 'auto':
            # è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•å¤šç§æ–¹æ³•å¹¶èåˆç»“æœ
            return self._predict_with_multi_methods(days_ahead)
        elif method == 'ml_only':
            # ä»…ä½¿ç”¨MLæ¨¡å‹
            try:
                if isinstance(self.best_model, dict):
                    return self._predict_with_ensemble_model(days_ahead)
                else:
                    return self._predict_with_single_model(days_ahead)
            except Exception as e:
                print(f"âŒ MLæ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")
                print("ğŸ“Š å›é€€åˆ°åŸºç¡€ç»Ÿè®¡æ–¹æ³•...")
                return self._basic_fallback_prediction(days_ahead)
        else:
            # å•ä¸€æ–¹æ³•
            method_map = {
                'lstm': self._predict_with_lstm,
                'arima': self._predict_with_arima,
                'prophet': self._predict_with_prophet,
                'basic': self._basic_fallback_prediction
            }
            
            if method in method_map:
                result = method_map[method](days_ahead)
                if result is not None:
                    return result
                else:
                    print(f"âš ï¸ {method}æ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ–¹æ³•")
                    return self._basic_fallback_prediction(days_ahead)
            else:
                print(f"âš ï¸ æœªçŸ¥é¢„æµ‹æ–¹æ³•: {method}ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•")
                return self._predict_with_multi_methods(days_ahead)
    
    def _predict_with_multi_methods(self, days_ahead):
        """å°è¯•å¤šç§é¢„æµ‹æ–¹æ³•å¹¶èåˆç»“æœ"""
        print("ğŸ¯ å°è¯•å¤šç§é¢„æµ‹æ–¹æ³•å¹¶èåˆç»“æœ...")
        
        prediction_results = {}
        
        # 1. å°è¯•MLæ¨¡å‹é¢„æµ‹
        try:
            if isinstance(self.best_model, dict):
                ml_result = self._predict_with_ensemble_model(days_ahead)
            else:
                ml_result = self._predict_with_single_model(days_ahead)
            
            if ml_result is not None:
                prediction_results['MLæ¨¡å‹'] = ml_result
        except Exception as e:
            print(f"âš ï¸ MLæ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # 2. å°è¯•ARIMAé¢„æµ‹
        arima_result = self._predict_with_arima(days_ahead)
        if arima_result is not None:
            prediction_results['ARIMA'] = arima_result
        
        # 3. å°è¯•Propheté¢„æµ‹
        prophet_result = self._predict_with_prophet(days_ahead)
        if prophet_result is not None:
            prediction_results['Prophet'] = prophet_result
        
        # 4. å°è¯•LSTMé¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        lstm_result = self._predict_with_lstm(days_ahead)
        if lstm_result is not None:
            prediction_results['LSTM'] = lstm_result
        
        # 5. åŸºç¡€ç»Ÿè®¡æ–¹æ³•ä½œä¸ºä¿åº•
        basic_result = self._basic_fallback_prediction(days_ahead)
        if basic_result is not None:
            prediction_results['åŸºç¡€ç»Ÿè®¡'] = basic_result
        
        # å¦‚æœæœ‰å¤šç§æ–¹æ³•æˆåŠŸï¼Œè¿›è¡Œèåˆ
        if len(prediction_results) > 1:
            return self._ensemble_predictions(prediction_results)
        elif len(prediction_results) == 1:
            # åªæœ‰ä¸€ç§æ–¹æ³•æˆåŠŸ
            method_name, result = list(prediction_results.items())[0]
            print(f"âœ… ä½¿ç”¨å•ä¸€æ–¹æ³•: {method_name}")
            return result
        else:
            # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†
            print("âŒ æ‰€æœ‰é¢„æµ‹æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„çº¿æ€§å¤–æ¨")
            return self._simple_linear_extrapolation(days_ahead)
    
    def _predict_with_single_model(self, days_ahead):
        """ä½¿ç”¨å•ä¸€æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        # è·å–æœ€åå‡ è¡Œçš„ç‰¹å¾ä½œä¸ºåŸºç¡€
        last_features = self.X_test.iloc[-days_ahead:].copy() if len(self.X_test) >= days_ahead else self.X.iloc[-days_ahead:].copy()
        
        predictions = []
        
        for i in range(days_ahead):
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
            if 'SVR' in str(type(self.best_model)):
                pred = self.best_model.predict(self.scaler.transform(last_features.iloc[[i % len(last_features)]]))
            else:
                pred = self.best_model.predict(last_features.iloc[[i % len(last_features)]])
            
            predictions.append(pred[0])
        
        # è®¡ç®—åŠ¨æ€ç½®ä¿¡åŒºé—´
        confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='ml')
        
        return predictions, confidence_intervals
    
    def _predict_with_ensemble_model(self, days_ahead):
        """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        ensemble_data = self.best_model
        ensemble_model = ensemble_data.get('model')
        
        
        if ensemble_model is None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŠ æƒå¹³å‡é›†æˆæ¨¡å‹
            if 'models' in ensemble_data and 'weights' in ensemble_data:
                print("âœ… æ‰¾åˆ°åŠ æƒå¹³å‡é›†æˆæ¨¡å‹")
                ensemble_model = ensemble_data  # ä½¿ç”¨æ•´ä¸ªå­—å…¸
            else:
                print("âš ï¸ é›†æˆæ¨¡å‹ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢„æµ‹å™¨ï¼Œå°è¯•å…¶ä»–é”®...")
                # å°è¯•å…¶ä»–å¯èƒ½çš„é”®
                for key in ['voting', 'weighted_average', 'ensemble_model']:
                    if key in ensemble_data:
                        ensemble_model = ensemble_data[key]
                        print(f"âœ… æ‰¾åˆ°é›†æˆæ¨¡å‹: {key}")
                        break
                
                if ensemble_model is None:
                    print("âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•")
                    return self._basic_fallback_prediction(days_ahead)
        
        # è·å–æœ€åå‡ è¡Œçš„ç‰¹å¾ä½œä¸ºåŸºç¡€
        last_features = self.X_test.iloc[-days_ahead:].copy() if len(self.X_test) >= days_ahead else self.X.iloc[-days_ahead:].copy()
        
        predictions = []
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åŠ æƒå¹³å‡é›†æˆï¼ˆå­—å…¸ç±»å‹ï¼‰
        if isinstance(ensemble_model, dict) and 'models' in ensemble_model:
            # åŠ æƒå¹³å‡é›†æˆ
            models = ensemble_model['models']
            weights = ensemble_model['weights']
            
            for i in range(days_ahead):
                feature_row = last_features.iloc[[i % len(last_features)]]
                weighted_pred = 0
                
                for model, weight in zip(models, weights):
                    if 'SVR' in str(type(model)):
                        pred = model.predict(self.scaler.transform(feature_row))
                    else:
                        pred = model.predict(feature_row)
                    weighted_pred += pred[0] * weight
                    
                predictions.append(weighted_pred)
        else:
            # æ ‡å‡†sklearné›†æˆæ¨¡å‹
            for i in range(days_ahead):
                pred = ensemble_model.predict(last_features.iloc[[i % len(last_features)]])
                predictions.append(pred[0])
        
        # è®¡ç®—åŠ¨æ€ç½®ä¿¡åŒºé—´
        confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='ml')
        
        return predictions, confidence_intervals
    
    def _calculate_confidence_intervals(self, predictions, historical_std, ci_factor=0.12):
        """è®¡ç®—é¢„æµ‹çš„ç½®ä¿¡åŒºé—´"""
        confidence_intervals = []
        for pred in predictions:
            ci_margin = 1.96 * historical_std * ci_factor
            confidence_intervals.append((pred - ci_margin, pred + ci_margin))
        return confidence_intervals
    
    def _generate_simple_interpretation(self):
        """ç”Ÿæˆç®€åŒ–çš„æ¨¡å‹è§£é‡ŠæŠ¥å‘Š"""
        print("\nğŸ” ç”Ÿæˆæ¨¡å‹å¯è§£é‡Šæ€§æŠ¥å‘Š...")
        
        # è·å–å¯è§£é‡Šçš„æ¨¡å‹
        interpretable_model = self._get_interpretable_model()
        
        if interpretable_model is None:
            print("âš ï¸ æ— æ³•è·å–å¯è§£é‡Šçš„æ¨¡å‹")
            return
        
        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        if hasattr(interpretable_model, 'feature_importances_'):
            print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ:")
            feature_names = self.X.columns.tolist()
            importances = interpretable_model.feature_importances_
            
            # æ’åºå¹¶æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
            importance_dict = dict(zip(feature_names, importances))
            sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
            
            for feature, importance in sorted_importance[:10]:
                print(f"  â€¢ {feature}: {importance:.4f}")
        else:
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        
        # æ¨¡å‹æ€§èƒ½æ‘˜è¦
        if hasattr(self, 'models') and self.models:
            print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æ‘˜è¦:")
            for name, scores in self.models.items():
                if name != 'Ensemble':
                    print(f"  â€¢ {name}: RÂ²={scores['r2']:.3f}, CV-RÂ²={scores['cv_r2_mean']:.3f}Â±{scores['cv_r2_std']:.3f}")
    
    def _get_interpretable_model(self):
        """è·å–å¯è§£é‡Šçš„æ¨¡å‹å¯¹è±¡"""
        best_model = self.best_model
        
        # å¦‚æœæœ€ä½³æ¨¡å‹æ˜¯é›†æˆæ¨¡å‹ï¼ˆå­—å…¸ç±»å‹ï¼‰
        if isinstance(best_model, dict):
            if 'model' in best_model:
                return best_model['model']
            else:
                # å¦‚æœæ˜¯é›†æˆæ¨¡å‹ï¼Œå°è¯•æ‰¾åˆ°æœ€å¥½çš„å•ä¸€æ¨¡å‹
                print("ğŸ” é›†æˆæ¨¡å‹æ£€æµ‹åˆ°ï¼Œä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹è¿›è¡Œè§£é‡Š...")
                if hasattr(self, 'models') and self.models:
                    # æ‰¾åˆ°éé›†æˆçš„æœ€ä½³æ¨¡å‹
                    single_models = {k: v for k, v in self.models.items() if k != 'Ensemble'}
                    if single_models:
                        best_single_name = max(single_models.keys(), key=lambda x: single_models[x]['cv_r2_mean'])
                        print(f"  ä½¿ç”¨ {best_single_name} è¿›è¡Œæ¨¡å‹è§£é‡Š")
                        return single_models[best_single_name]['model']
                return None
        else:
            return best_model
    
    def _predict_with_lstm(self, days_ahead):
        """ä½¿ç”¨LSTMæ·±åº¦å­¦ä¹ é¢„æµ‹"""
        if not HAS_TENSORFLOW:
            print("âš ï¸ TensorFlowæœªå®‰è£…ï¼Œè·³è¿‡LSTMé¢„æµ‹")
            return None
            
        try:
            print("ğŸ§  ä½¿ç”¨LSTMæ·±åº¦å­¦ä¹ é¢„æµ‹...")
            
            from sklearn.preprocessing import MinMaxScaler
            
            # å‡†å¤‡LSTMæ•°æ®
            price_data = self.df['price_per_unit'].values
            lstm_scaler = MinMaxScaler()
            scaled_data = lstm_scaler.fit_transform(price_data.reshape(-1, 1))
            
            # åˆ›å»ºåºåˆ—æ•°æ®
            sequence_length = min(60, len(scaled_data) // 4)
            if sequence_length < 10:
                print("âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡LSTMé¢„æµ‹")
                return None
                
            X_lstm, y_lstm = [], []
            for i in range(sequence_length, len(scaled_data)):
                X_lstm.append(scaled_data[i-sequence_length:i, 0])
                y_lstm.append(scaled_data[i, 0])
            
            if len(X_lstm) < 50:
                print("âš ï¸ åºåˆ—æ•°æ®ä¸è¶³ï¼Œè·³è¿‡LSTMé¢„æµ‹")
                return None
            
            X_lstm = np.array(X_lstm).reshape((len(X_lstm), sequence_length, 1))
            y_lstm = np.array(y_lstm)
            
            # æ„å»ºLSTMæ¨¡å‹
            from tensorflow.keras.models import Sequential
            from tensorflow.keras.layers import LSTM, Dense, Dropout
            from tensorflow.keras.optimizers import Adam
            
            model = Sequential([
                LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),
                Dropout(0.2),
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25),
                Dense(1)
            ])
            
            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
            
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_lstm, y_lstm, epochs=50, batch_size=32, verbose=0)
            
            # è¿›è¡Œé¢„æµ‹
            last_sequence = scaled_data[-sequence_length:].reshape(1, sequence_length, 1)
            predictions = []
            
            for _ in range(days_ahead):
                pred = model.predict(last_sequence, verbose=0)
                predictions.append(pred[0, 0])
                
                # æ›´æ–°åºåˆ—
                last_sequence = np.roll(last_sequence, -1, axis=1)
                last_sequence[0, -1, 0] = pred[0, 0]
            
            # åå‘ç¼©æ”¾
            predictions = lstm_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='lstm')
            
            return predictions.tolist(), confidence_intervals
            
        except Exception as e:
            print(f"âŒ LSTMé¢„æµ‹å¤±è´¥: {str(e)}")
            return None
    
    def _predict_with_arima(self, days_ahead):
        """ä½¿ç”¨ARIMAæ—¶é—´åºåˆ—é¢„æµ‹"""
        if not HAS_STATSMODELS:
            print("âš ï¸ statsmodelsæœªå®‰è£…ï¼Œè·³è¿‡ARIMAé¢„æµ‹")
            return None
            
        try:
            print("ğŸ“ˆ ä½¿ç”¨ARIMAæ—¶é—´åºåˆ—é¢„æµ‹...")
            
            # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
            ts_data = self.df['price_per_unit'].dropna()
            if len(ts_data) < 50:
                print("âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡ARIMAé¢„æµ‹")
                return None
            
            # æ‹ŸåˆARIMAæ¨¡å‹
            model = ARIMA(ts_data, order=(1, 1, 1))
            fitted_model = model.fit()
            
            # è¿›è¡Œé¢„æµ‹
            forecast = fitted_model.forecast(steps=days_ahead)
            predictions = forecast.tolist()
            
            # ä½¿ç”¨æˆ‘ä»¬ç»Ÿä¸€çš„åŠ¨æ€ç½®ä¿¡åŒºé—´è®¡ç®—ï¼Œè€Œä¸æ˜¯ARIMAè‡ªå¸¦çš„
            confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='arima')
            
            return predictions, confidence_intervals
            
        except Exception as e:
            print(f"âŒ ARIMAé¢„æµ‹å¤±è´¥: {str(e)}")
            return None
    
    def _predict_with_prophet(self, days_ahead):
        """ä½¿ç”¨Prophetæ—¶é—´åºåˆ—é¢„æµ‹"""
        if not HAS_PROPHET:
            print("âš ï¸ Prophetæœªå®‰è£…ï¼Œè·³è¿‡Propheté¢„æµ‹")
            return None
            
        try:
            print("ğŸ”® ä½¿ç”¨Prophetæ—¶é—´åºåˆ—é¢„æµ‹...")
            
            # å‡†å¤‡Prophetæ•°æ®
            prophet_df = self.df[['created_at', 'price_per_unit']].copy()
            prophet_df.columns = ['ds', 'y']
            prophet_df = prophet_df.dropna()
            
            if len(prophet_df) < 50:
                print("âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡Propheté¢„æµ‹")
                return None
            
            # åˆ›å»ºå’Œè®­ç»ƒProphetæ¨¡å‹
            model = Prophet(
                daily_seasonality=True,
                weekly_seasonality=True,
                yearly_seasonality=False,
                interval_width=0.95
            )
            
            model.fit(prophet_df)
            
            # åˆ›å»ºæœªæ¥æ•°æ®æ¡†
            future = model.make_future_dataframe(periods=days_ahead, freq='D')
            forecast = model.predict(future)
            
            # æå–é¢„æµ‹ç»“æœ
            predictions = forecast['yhat'].tail(days_ahead).tolist()
            
            # ä½¿ç”¨æˆ‘ä»¬ç»Ÿä¸€çš„åŠ¨æ€ç½®ä¿¡åŒºé—´è®¡ç®—ï¼Œè€Œä¸æ˜¯Prophetè‡ªå¸¦çš„
            confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='prophet')
            
            return predictions, confidence_intervals
            
        except Exception as e:
            print(f"âŒ Propheté¢„æµ‹å¤±è´¥: {str(e)}")
            return None
    
    def _calculate_dynamic_confidence_intervals(self, predictions, method='default'):
        """åŠ¨æ€è®¡ç®—ç½®ä¿¡åŒºé—´"""
        try:
            # æ–¹æ³•1ï¼šåŸºäºæ®‹å·®åˆ†å¸ƒï¼ˆæœ€å‡†ç¡®çš„æ–¹æ³•ï¼‰
            if hasattr(self, 'best_model') and self.best_model is not None:
                # è·å–æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ®‹å·®
                interpretable_model = self._get_interpretable_model()
                if interpretable_model is not None and hasattr(self, 'y_test'):
                    try:
                        if 'SVR' in str(type(interpretable_model)):
                            y_pred = interpretable_model.predict(self.X_test_scaled)
                        else:
                            y_pred = interpretable_model.predict(self.X_test)
                        
                        residuals = self.y_test - y_pred
                        residual_std = np.std(residuals)
                        residual_mean = np.mean(residuals)
                        
                        # ä½¿ç”¨æ®‹å·®åˆ†å¸ƒè®¡ç®—ç½®ä¿¡åŒºé—´
                        confidence_intervals = []
                        for i, pred in enumerate(predictions):
                            # è€ƒè™‘æ®‹å·®çš„åå·®å’Œæ ‡å‡†å·®ï¼ŒåŠ ä¸Šè½»å¾®çš„æ—¶é—´è¡°å‡
                            time_factor = 1 + (i * 0.01)  # æ¯å¤©å¢åŠ 1%çš„ä¸ç¡®å®šæ€§ï¼ˆæ›´ä¿å®ˆï¼‰
                            adjusted_std = residual_std * time_factor
                            
                            ci_lower = pred + residual_mean - 1.96 * adjusted_std
                            ci_upper = pred + residual_mean + 1.96 * adjusted_std
                            confidence_intervals.append((ci_lower, ci_upper))
                        
                        return confidence_intervals
                    except Exception as e:
                        print(f"âš ï¸ æ®‹å·®è®¡ç®—å¤±è´¥: {str(e)}")
                        pass
            
            # æ–¹æ³•2ï¼šåŸºäºå†å²ä»·æ ¼æ³¢åŠ¨ï¼ˆå›é€€æ–¹æ³•ï¼‰
            historical_prices = self.df['price_per_unit'].values
            recent_prices = historical_prices[-min(30, len(historical_prices)):]
            
            # è®¡ç®—æ›´ä¿å®ˆçš„æ³¢åŠ¨æ€§ä¼°è®¡
            price_changes = np.diff(recent_prices)
            daily_volatility = np.std(price_changes)
            
            # æ ¹æ®é¢„æµ‹æ–¹æ³•è°ƒæ•´ç½®ä¿¡åŒºé—´å®½åº¦
            method_factors = {
                'lstm': 2.0,       # LSTM: 2å€æ—¥æ³¢åŠ¨æ€§
                'arima': 1.5,      # ARIMA: 1.5å€æ—¥æ³¢åŠ¨æ€§  
                'prophet': 1.8,    # Prophet: 1.8å€æ—¥æ³¢åŠ¨æ€§
                'ml': 1.2,         # MLæ¨¡å‹: 1.2å€æ—¥æ³¢åŠ¨æ€§ï¼ˆæœ€ä¿å®ˆï¼‰
                'default': 1.5
            }
            
            volatility_multiplier = method_factors.get(method, 1.5)
            
            confidence_intervals = []
            for i, pred in enumerate(predictions):
                # è½»å¾®çš„æ—¶é—´è¡°å‡
                time_factor = 1 + (i * 0.05)  # æ¯å¤©å¢åŠ 5%çš„ä¸ç¡®å®šæ€§
                
                # ä½¿ç”¨æ—¥æ³¢åŠ¨æ€§ä½œä¸ºåŸºç¡€
                ci_margin = 1.96 * daily_volatility * volatility_multiplier * time_factor
                
                confidence_intervals.append((pred - ci_margin, pred + ci_margin))
            
            return confidence_intervals
            
        except Exception as e:
            print(f"âš ï¸ åŠ¨æ€ç½®ä¿¡åŒºé—´è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•: {str(e)}")
            return self._calculate_confidence_intervals(predictions, self.y.std())
    
    def _ensemble_predictions(self, prediction_results):
        """èåˆå¤šç§é¢„æµ‹æ–¹æ³•çš„ç»“æœ"""
        print(f"ğŸ”„ èåˆ {len(prediction_results)} ç§é¢„æµ‹æ–¹æ³•çš„ç»“æœ...")
        
        if not prediction_results:
            return None, None
        
        # æ˜¾ç¤ºå„æ–¹æ³•çš„é¢„æµ‹èŒƒå›´
        for method_name, (predictions, _) in prediction_results.items():
            if predictions:
                print(f"   - {method_name}: é¢„æµ‹èŒƒå›´ {min(predictions):.2f} - {max(predictions):.2f}")
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºæ–¹æ³•çš„å¯é æ€§ï¼‰
        method_weights = {
            'MLæ¨¡å‹': 0.35,      # æœ€é«˜æƒé‡ç»™MLæ¨¡å‹
            'ARIMA': 0.25,       # ARIMAé€‚åˆæ—¶é—´åºåˆ—
            'Prophet': 0.20,     # Propheté€‚åˆè¶‹åŠ¿åˆ†æ
            'LSTM': 0.15,        # LSTMé€‚åˆå¤æ‚æ¨¡å¼
            'åŸºç¡€ç»Ÿè®¡': 0.05     # æœ€ä½æƒé‡ç»™åŸºç¡€æ–¹æ³•
        }
        
        # æ ‡å‡†åŒ–æƒé‡ï¼ˆåªå¯¹å®é™…å­˜åœ¨çš„æ–¹æ³•ï¼‰
        available_methods = list(prediction_results.keys())
        total_weight = sum(method_weights.get(method, 0.1) for method in available_methods)
        normalized_weights = {method: method_weights.get(method, 0.1) / total_weight 
                            for method in available_methods}
        
        # èåˆé¢„æµ‹ç»“æœ
        days_ahead = len(list(prediction_results.values())[0][0])
        ensemble_predictions = []
        ensemble_confidence_intervals = []
        
        for day in range(days_ahead):
            # åŠ æƒå¹³å‡é¢„æµ‹å€¼
            weighted_pred = 0
            
            # æ”¶é›†å„æ–¹æ³•çš„é¢„æµ‹å’Œä¸ç¡®å®šæ€§
            method_predictions = []
            method_uncertainties = []
            
            for method_name, (predictions, confidence_intervals) in prediction_results.items():
                weight = normalized_weights[method_name]
                weighted_pred += weight * predictions[day]
                
                # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„ä¸ç¡®å®šæ€§ï¼ˆåŠå®½åº¦ï¼‰
                uncertainty = (confidence_intervals[day][1] - confidence_intervals[day][0]) / 2
                method_predictions.append(predictions[day])
                method_uncertainties.append(uncertainty)
            
            # è®¡ç®—èåˆåçš„ä¸ç¡®å®šæ€§ï¼šä½¿ç”¨åŠ æƒå¹³å‡çš„ä¸ç¡®å®šæ€§ï¼Œè€Œä¸æ˜¯è¾¹ç•Œçš„åŠ æƒå¹³å‡
            weighted_uncertainty = sum(normalized_weights[method_name] * method_uncertainties[i] 
                                     for i, method_name in enumerate(prediction_results.keys()))
            
            # æ·»åŠ æ–¹æ³•é—´å·®å¼‚çš„ä¸ç¡®å®šæ€§ï¼ˆé¢„æµ‹åˆ†æ­§åº¦ï¼‰
            if len(method_predictions) > 1:
                prediction_spread = np.std(method_predictions)
                total_uncertainty = np.sqrt(weighted_uncertainty**2 + (prediction_spread * 0.5)**2)
            else:
                total_uncertainty = weighted_uncertainty
            
            
            ensemble_predictions.append(weighted_pred)
            ensemble_confidence_intervals.append((weighted_pred - total_uncertainty, weighted_pred + total_uncertainty))
        
        print(f"âœ… èåˆé¢„æµ‹å®Œæˆï¼Œæœ€ç»ˆèŒƒå›´: {min(ensemble_predictions):.2f} - {max(ensemble_predictions):.2f}")
        
        return ensemble_predictions, ensemble_confidence_intervals
    
    def _simple_linear_extrapolation(self, days_ahead):
        """ç®€å•çš„çº¿æ€§å¤–æ¨ä½œä¸ºæœ€åä¿åº•æ–¹æ³•"""
        print("ğŸ“ˆ ä½¿ç”¨ç®€å•çº¿æ€§å¤–æ¨...")
        
        try:
            historical_prices = self.df['price_per_unit'].values
            recent_prices = historical_prices[-min(10, len(historical_prices)):]
            
            if len(recent_prices) < 2:
                # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œè¿”å›æœ€åä¸€ä¸ªä»·æ ¼
                last_price = historical_prices[-1] if len(historical_prices) > 0 else 40.0
                predictions = [last_price] * days_ahead
            else:
                # æ‹Ÿåˆçº¿æ€§è¶‹åŠ¿
                x = np.arange(len(recent_prices))
                coeffs = np.polyfit(x, recent_prices, 1)
                trend = coeffs[0]
                intercept = coeffs[1]
                
                # å¤–æ¨é¢„æµ‹
                predictions = []
                for day in range(1, days_ahead + 1):
                    pred = intercept + trend * (len(recent_prices) + day - 1)
                    predictions.append(max(pred, 0.01))  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
            
            # ç®€å•çš„ç½®ä¿¡åŒºé—´
            volatility = np.std(recent_prices) if len(recent_prices) > 1 else 5.0
            confidence_intervals = []
            for pred in predictions:
                margin = 1.96 * volatility * 0.2  # 20%çš„æ³¢åŠ¨æ€§å› å­
                confidence_intervals.append((pred - margin, pred + margin))
            
            return predictions, confidence_intervals
            
        except Exception as e:
            print(f"âŒ çº¿æ€§å¤–æ¨å¤±è´¥: {str(e)}")
            # æœ€åçš„æœ€åä¿åº•æ–¹æ³•
            last_price = 40.0  # ç¡¬ç¼–ç ä¸€ä¸ªåˆç†çš„ä»·æ ¼
            predictions = [last_price] * days_ahead
            confidence_intervals = [(last_price - 5, last_price + 5)] * days_ahead
            return predictions, confidence_intervals
    
    def _basic_fallback_prediction(self, days_ahead):
        """åŸºç¡€å¤‡é€‰é¢„æµ‹æ–¹æ³•"""
        print("ğŸ“Š ä½¿ç”¨åŸºç¡€ç»Ÿè®¡æ–¹æ³•é¢„æµ‹...")
        
        # è·å–å†å²ä»·æ ¼æ•°æ®
        historical_prices = self.df['price_per_unit'].values
        recent_prices = historical_prices[-min(30, len(historical_prices)):]
        
        if len(recent_prices) > 1:
            trend = np.polyfit(range(len(recent_prices)), recent_prices, 1)[0]
            volatility = np.std(recent_prices)
            mean_price = np.mean(recent_prices)
        else:
            trend, volatility, mean_price = 0, 0, historical_prices[-1]
        
        print(f"ğŸ“Š ä»·æ ¼è¶‹åŠ¿åˆ†æ:")
        print(f"  æœ€è¿‘ä»·æ ¼å‡å€¼: {mean_price:.2f}")
        print(f"  ä»·æ ¼è¶‹åŠ¿: {trend:+.4f} æ¯å•ä½æ—¶é—´")
        print(f"  ä»·æ ¼æ³¢åŠ¨æ€§: {volatility:.2f}")
        
        predictions, confidence_intervals = [], []
        
        for day in range(days_ahead):
            # åŸºç¡€é¢„æµ‹
            base_prediction = mean_price + trend * (day + 1)
            cycle_adjustment = volatility * 0.05 * np.sin(2 * np.pi * day / 7) if volatility > 0 else 0
            deterministic_noise = np.sin(day * 0.7) * volatility * 0.02 if volatility > 0 else 0
            prediction = max(base_prediction + cycle_adjustment + deterministic_noise, 0.01)
            
            predictions.append(prediction)
            
        # è®¡ç®—åŠ¨æ€ç½®ä¿¡åŒºé—´
        confidence_intervals = self._calculate_dynamic_confidence_intervals(predictions, method='default')
        
        return predictions, confidence_intervals
    
    def plot_analysis(self):
        """ç»˜åˆ¶åˆ†æå›¾è¡¨"""
        print("ğŸ“Š ç»˜åˆ¶åˆ†æå›¾è¡¨...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä»·æ ¼æ—¶é—´åºåˆ—
        axes[0, 0].plot(self.df['created_at'], self.df['price_per_unit'], alpha=0.7)
        axes[0, 0].set_title('Price Time Series', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Date')
        axes[0, 0].set_ylabel('Price per Unit')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ä»·æ ¼åˆ†å¸ƒ
        axes[0, 1].hist(self.df['price_per_unit'], bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('Price Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Price per Unit')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        if self.models:
            model_names = list(self.models.keys())
            r2_scores = [self.models[name]['r2'] for name in model_names]
            
            bars = axes[1, 0].bar(model_names, r2_scores, alpha=0.7)
            axes[1, 0].set_title('Model Performance (RÂ² Score)', fontsize=14, fontweight='bold')
            axes[1, 0].set_ylabel('RÂ² Score')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, r2_scores):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.3f}', ha='center', va='bottom')
        
        # 4. ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            top_features = self.feature_importance.head(10)
            axes[1, 1].barh(range(len(top_features)), top_features['importance'])
            axes[1, 1].set_yticks(range(len(top_features)))
            axes[1, 1].set_yticklabels(top_features['feature'])
            axes[1, 1].set_title('Top 10 Feature Importance', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('Importance')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_predictions(self, predictions_df):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœ - æ”¹è¿›ç‰ˆ"""
        print("ğŸ“ˆ ç»˜åˆ¶é¢„æµ‹ç»“æœ...")
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 12))
        
        # 1. å†å²ä»·æ ¼å’Œé¢„æµ‹ï¼ˆå¸¦ç½®ä¿¡åŒºé—´ï¼‰
        axes[0].plot(self.df['created_at'], self.df['price_per_unit'], 
                    label='Historical Price', alpha=0.7, linewidth=1, color='blue')
        
        # é¢„æµ‹ä»·æ ¼
        axes[0].plot(predictions_df['date'], predictions_df['predicted_price'], 
                    label='Predicted Price', color='red', linewidth=2, linestyle='-', marker='o')
        
        # ç½®ä¿¡åŒºé—´
        if 'ci_lower' in predictions_df.columns:
            axes[0].fill_between(predictions_df['date'], 
                               predictions_df['ci_lower'], 
                               predictions_df['ci_upper'], 
                               alpha=0.3, color='red', label='95% Confidence Interval')
        
        axes[0].set_title('Price History and Future Prediction with Confidence Interval', 
                         fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Date')
        axes[0].set_ylabel('Price per Unit')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        axes[0].tick_params(axis='x', rotation=45)
        
        # 2. é¢„æµ‹è¶‹åŠ¿åˆ†æï¼ˆæ›´è¯¦ç»†ï¼‰
        axes[1].plot(predictions_df['date'], predictions_df['predicted_price'], 
                    marker='o', linewidth=3, markersize=8, color='red', label='Predicted Price')
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(predictions_df) > 1:
            x_numeric = range(len(predictions_df))
            z = np.polyfit(x_numeric, predictions_df['predicted_price'], 1)
            trend_line = np.poly1d(z)
            axes[1].plot(predictions_df['date'], trend_line(x_numeric), 
                        '--', color='orange', linewidth=2, alpha=0.8, 
                        label=f'Trend: {"ä¸Šå‡" if z[0] > 0 else "ä¸‹é™"} ({z[0]:.2f}/day)')
        
        axes[1].set_title('Future Price Trend Analysis', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Date')
        axes[1].set_ylabel('Predicted Price')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        current_price = self.df['price_per_unit'].iloc[-1]
        future_price = predictions_df['predicted_price'].iloc[-1]
        price_change = future_price - current_price
        price_change_pct = (price_change / current_price) * 100
        
        # è®¡ç®—ä»·æ ¼èŒƒå›´
        min_pred = predictions_df['predicted_price'].min()
        max_pred = predictions_df['predicted_price'].max()
        price_range = max_pred - min_pred
        
        stats_text = f'''Current: {current_price:.2f}
Future: {future_price:.2f}
Change: {price_change:+.2f} ({price_change_pct:+.1f}%)
Range: {min_pred:.2f} - {max_pred:.2f}
Volatility: {price_range:.2f}'''
        
        axes[1].text(0.02, 0.98, stats_text, transform=axes[1].transAxes, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†é¢„æµ‹ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†é¢„æµ‹ç»“æœ:")
        print(f"å½“å‰ä»·æ ¼: {current_price:.2f}")
        print(f"é¢„æµ‹ä»·æ ¼èŒƒå›´: {min_pred:.2f} - {max_pred:.2f}")
        print(f"æœ€ç»ˆé¢„æµ‹ä»·æ ¼: {future_price:.2f}")
        print(f"ä»·æ ¼å˜åŒ–: {price_change:+.2f} ({price_change_pct:+.1f}%)")
        print(f"é¢„æµ‹æ³¢åŠ¨æ€§: {price_range:.2f}")
        
        # æ˜¾ç¤ºæ¯æ—¥é¢„æµ‹
        print(f"\nğŸ“… æ¯æ—¥é¢„æµ‹è¯¦æƒ…:")
        for i, row in predictions_df.iterrows():
            print(f"  {row['date'].strftime('%Y-%m-%d')}: {row['predicted_price']:.2f}")
        
        return predictions_df
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""
        ========================================
        æœºå™¨å­¦ä¹ ä»·æ ¼é¢„æµ‹åˆ†ææŠ¥å‘Š
        ========================================
        
        æ•°æ®æ¦‚è§ˆ:
        - æ€»è®°å½•æ•°: {len(self.df)}
        - æ—¶é—´èŒƒå›´: {self.df['created_at'].min()} åˆ° {self.df['created_at'].max()}
        - ç‰¹å¾æ•°é‡: {len(self.X.columns)}
        
        æ¨¡å‹æ€§èƒ½:
        """
        
        if self.models:
            for name, metrics in self.models.items():
                report += f"        - {name}: RÂ²={metrics['r2']:.3f}, RMSE={metrics['rmse']:.2f}\n"
        
        if self.feature_importance is not None:
            report += f"\n        é‡è¦ç‰¹å¾ (å‰5ä¸ª):\n"
            for i, row in self.feature_importance.head(5).iterrows():
                report += f"        - {row['feature']}: {row['importance']:.3f}\n"
        
        report += f"\n        æ•°æ®è´¨é‡:\n"
        report += f"        - ç¼ºå¤±å€¼: {self.df.isnull().sum().sum()}\n"
        report += f"        - å¼‚å¸¸å€¼: å·²å¤„ç†\n"
        report += f"        - æ•°æ®å®Œæ•´æ€§: {((len(self.df) - self.df.isnull().sum().sum()) / (len(self.df) * len(self.df.columns)) * 100):.1f}%\n"
        
        print(report)
        return report
    
    def run_complete_analysis(self, days_ahead=7):
        """è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„æœºå™¨å­¦ä¹ åˆ†æ...")
        print("=" * 50)
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        if not self.load_and_prepare_data():
            return None
        
        # 2. è®­ç»ƒæ¨¡å‹
        model_scores = self.train_models()
        
        # 3. è¶…å‚æ•°è°ƒä¼˜
        self.hyperparameter_tuning()
        
        # 4. ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.analyze_feature_importance()
        
        # 5. æ¨¡å‹ç¨³å®šæ€§æµ‹è¯•
        stability_results = self.test_model_stability(n_runs=3)
        
        # 6. é¢„æµ‹æœªæ¥ä»·æ ¼
        predictions = self.predict_future_prices(days_ahead)
        
        # 6. ç»˜åˆ¶åˆ†æå›¾è¡¨
        self.plot_analysis()
        
        # 7. ç»˜åˆ¶é¢„æµ‹ç»“æœ
        if predictions is not None:
            self.plot_predictions(predictions)
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print("âœ… æœºå™¨å­¦ä¹ åˆ†æå®Œæˆ!")
        return predictions


def load_items_config():
    """
    åŠ è½½ç‰©å“é…ç½®æ–‡ä»¶
    """
    import os
    import json
    
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    config_path = os.path.join(project_root, 'items_config.json')
    
    # é»˜è®¤é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
    default_config = {
        "items": [
            {"name": "Gold Ore", "file": "Gold_Ore_API.py", "csv": "gold_ore.csv", "category": "ore", "enabled": True},
            {"name": "Iron Ore", "file": "Iron_Ore_API.py", "csv": "iron_ore.csv", "category": "ore", "enabled": True},
            {"name": "Cobalt Ore", "file": "Cobalt_Ore_API.py", "csv": "cobalt_ore.csv", "category": "ore", "enabled": True}
        ],
        "auto_discovery": {"enabled": True, "ore_directory": "Ore", "naming_pattern": "*_API.py"}
    }
    
    try:
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        else:
            return default_config
    except Exception as e:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}")
        return default_config


def get_available_items():
    """
    åŠ¨æ€è·å–å¯ç”¨ç‰©å“åˆ—è¡¨ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶å’Œè‡ªåŠ¨å‘ç°
    """
    import os
    import glob
    
    # åŠ è½½é…ç½®
    config = load_items_config()
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    available_items = {}
    item_index = 1
    
    # 1. ä»é…ç½®æ–‡ä»¶åŠ è½½å·²å¯ç”¨çš„ç‰©å“
    for item in config.get("items", []):
        if not item.get("enabled", True):
            continue
            
        # æ£€æŸ¥APIæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        category = item.get("category", "ore")
        directory = "src/api"  # ç»Ÿä¸€ä½¿ç”¨src/apiç›®å½•
        api_path = os.path.join(project_root, directory, item["file"])
        
        if os.path.exists(api_path):
            item_info = {
                "name": item["name"],
                "file": item["file"],
                "csv": item["csv"],
                "category": item.get("category", "ore"),
                "description": item.get("description", ""),
                "directory": directory
            }
            available_items[item_index] = item_info
            item_index += 1
        else:
            print(f"âš ï¸ APIæ–‡ä»¶ä¸å­˜åœ¨: {api_path}")
    
    # 2. è‡ªåŠ¨å‘ç°æ–°ç‰©å“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    auto_discovery = config.get("auto_discovery", {})
    if auto_discovery.get("enabled", True):
        directories_to_scan = ["src/api"]
        
        for directory in directories_to_scan:
            dir_path = os.path.join(project_root, directory)
            if not os.path.exists(dir_path):
                continue
                
            pattern = auto_discovery.get("naming_pattern", "*_API.py")
            api_files = glob.glob(os.path.join(dir_path, pattern))
            
            # è·å–å·²çŸ¥çš„æ–‡ä»¶ååˆ—è¡¨
            known_files = [item["file"] for item in config.get("items", [])]
            
            for api_file in api_files:
                filename = os.path.basename(api_file)
                
                # è·³è¿‡å·²é…ç½®çš„ç‰©å“
                if filename in known_files:
                    continue
                
                # è§£æç‰©å“åç§°å’ŒCSVåç§°
                item_name = filename.replace("_API.py", "").replace("_", " ")
                csv_name = filename.replace("_API.py", ".csv").lower()
                
                # æ ¹æ®æ–‡ä»¶åç¡®å®šç±»åˆ« (é»˜è®¤ä¸ºore)
                category = "consumable" if "potion" in item_name.lower() or "consumable" in filename.lower() else "ore"
                
                new_item = {
                    "name": item_name,
                    "file": filename,
                    "csv": csv_name,
                    "category": category,
                    "description": f"è‡ªåŠ¨å‘ç°çš„{item_name}",
                    "directory": directory
                }
                
                available_items[item_index] = new_item
                item_index += 1
                print(f"ğŸ†• è‡ªåŠ¨å‘ç°æ–°ç‰©å“: {item_name} ({category})")
    
    return available_items


def add_new_item(name, api_file, csv_file, category="ore", description="", enabled=True):
    """
    æ·»åŠ æ–°ç‰©å“åˆ°é…ç½®æ–‡ä»¶
    
    Args:
        name: ç‰©å“åç§° (ä¾‹: "Silver Ore")
        api_file: APIæ–‡ä»¶å (ä¾‹: "Silver_Ore_API.py")
        csv_file: CSVæ–‡ä»¶å (ä¾‹: "silver_ore.csv")
        category: ç‰©å“ç±»åˆ« ("ore", "consumable", "equipment", "material")
        description: ç‰©å“æè¿°
        enabled: æ˜¯å¦å¯ç”¨
    """
    import os
    import json
    
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    config_path = os.path.join(project_root, 'items_config.json')
    
    # åŠ è½½ç°æœ‰é…ç½®
    config = load_items_config()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    for item in config.get("items", []):
        if item["name"] == name or item["file"] == api_file:
            print(f"âš ï¸ ç‰©å“ '{name}' å·²å­˜åœ¨!")
            return False
    
    # æ·»åŠ æ–°ç‰©å“
    new_item = {
        "name": name,
        "file": api_file,
        "csv": csv_file,
        "category": category,
        "description": description,
        "enabled": enabled
    }
    
    config.setdefault("items", []).append(new_item)
    
    # ä¿å­˜é…ç½®
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"âœ… æˆåŠŸæ·»åŠ æ–°ç‰©å“: {name}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")
        return False


def list_items_config():
    """
    æ˜¾ç¤ºå½“å‰ç‰©å“é…ç½®
    """
    config = load_items_config()
    
    print("\nğŸ“‹ å½“å‰ç‰©å“é…ç½®:")
    print("=" * 50)
    
    for i, item in enumerate(config.get("items", []), 1):
        status = "âœ…" if item.get("enabled", True) else "âŒ"
        category = item.get("category", "unknown")
        description = item.get("description", "")
        
        print(f"{i:2d}. {status} {item['name']} ({category})")
        print(f"     API: {item['file']}")
        print(f"     CSV: {item['csv']}")
        if description:
            print(f"     æè¿°: {description}")
        print()
    
    auto_discovery = config.get("auto_discovery", {})
    print(f"ğŸ” è‡ªåŠ¨å‘ç°: {'å¯ç”¨' if auto_discovery.get('enabled') else 'ç¦ç”¨'}")


def manage_items_config():
    """
    ç‰©å“é…ç½®ç®¡ç†èœå•
    """
    while True:
        print("\nğŸ› ï¸ ç‰©å“é…ç½®ç®¡ç†")
        print("=" * 30)
        print("1. æŸ¥çœ‹å½“å‰é…ç½®")
        print("2. æ·»åŠ æ–°ç‰©å“")
        print("3. è¿”å›ä¸»èœå•")
        
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
        
        if choice == "1":
            list_items_config()
        elif choice == "2":
            print("\nâ• æ·»åŠ æ–°ç‰©å“")
            name = input("ç‰©å“åç§° (ä¾‹: Silver Ore): ").strip()
            if not name:
                print("âŒ ç‰©å“åç§°ä¸èƒ½ä¸ºç©º")
                continue
                
            api_file = input("APIæ–‡ä»¶å (ä¾‹: Silver_Ore_API.py): ").strip()
            if not api_file.endswith("_API.py"):
                api_file += "_API.py"
                
            csv_file = input(f"CSVæ–‡ä»¶å (é»˜è®¤: {name.lower().replace(' ', '_')}.csv): ").strip()
            if not csv_file:
                csv_file = f"{name.lower().replace(' ', '_')}.csv"
                
            category = input("ç±»åˆ« (ore/consumable/equipment/material, é»˜è®¤: ore): ").strip()
            if not category:
                category = "ore"
                
            description = input("æè¿° (å¯é€‰): ").strip()
            
            add_new_item(name, api_file, csv_file, category, description)
        elif choice == "3":
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")


def collect_and_analyze():
    """
    é›†æˆåŠŸèƒ½ï¼šé€‰æ‹©ç‰©å“ â†’ è°ƒç”¨APIè·å–æ•°æ® â†’ ç”ŸæˆCSV â†’ è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ
    æ”¯æŒè‡ªå®šä¹‰ç‰©å“å’ŒAPIæ–‡ä»¶
    """
    import os
    import sys
    import subprocess
    
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½å¸‚åœºåˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # 1. åŠ¨æ€è·å–å¯ç”¨ç‰©å“åˆ—è¡¨
    available_items = get_available_items()
    
    print("\nğŸ“¦ å¯ç”¨ç‰©å“åˆ—è¡¨:")
    for key, item in available_items.items():
        category_emoji = "â›ï¸" if item.get('category') == 'ore' else "ğŸ§ª"
        description = item.get('description', '')
        if description:
            print(f"  {key}. {category_emoji} {item['name']} - {description}")
        else:
            print(f"  {key}. {category_emoji} {item['name']}")
    
    # 2. ç”¨æˆ·é€‰æ‹©ç‰©å“
    try:
        choice = int(input("\nè¯·é€‰æ‹©è¦åˆ†æçš„ç‰©å“ (è¾“å…¥æ•°å­—): "))
        if choice not in available_items:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹: Gold Ore")
            choice = 1
        
        selected_item = available_items[choice]
        print(f"âœ… å·²é€‰æ‹©: {selected_item['name']} ({selected_item.get('category', 'unknown')})")
        
    except ValueError:
        print("âŒ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹: Gold Ore")
        choice = 1
        selected_item = available_items[1]
    
    # 3. è¯¢é—®æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®
    update_data = input("\nğŸ”„ æ˜¯å¦éœ€è¦è·å–æœ€æ–°æ•°æ®? (y/n, é»˜è®¤n): ").lower().strip()
    
    if update_data in ['y', 'yes', 'æ˜¯']:
        print(f"\nğŸ“¡ æ­£åœ¨è°ƒç”¨ {selected_item['name']} API è·å–æœ€æ–°æ•°æ®...")
        
        try:
            # æ„å»ºAPIæ–‡ä»¶è·¯å¾„ - ç»Ÿä¸€ä½¿ç”¨src/apiç›®å½•
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            api_directory = "src/api"  # ç»Ÿä¸€ä½¿ç”¨src/apiç›®å½•
            api_file_path = os.path.join(project_root, api_directory, selected_item['file'])
            
            if not os.path.exists(api_file_path):
                print(f"âŒ APIæ–‡ä»¶ä¸å­˜åœ¨: {api_file_path}")
                return None
            
            print(f"ğŸ”„ æ‰§è¡Œ: {api_file_path}")
            
            # æ‰§è¡ŒAPIè„šæœ¬
            result = subprocess.run([sys.executable, api_file_path], 
                                  capture_output=True, text=True, 
                                  cwd=os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
            
            if result.returncode == 0:
                print("âœ… æ•°æ®è·å–å®Œæˆ!")
                print("ğŸ“Š APIè¾“å‡ºæ‘˜è¦:")
                # æ˜¾ç¤ºæœ€åå‡ è¡Œè¾“å‡º
                output_lines = result.stdout.strip().split('\n')
                for line in output_lines[-5:]:
                    if line.strip():
                        print(f"  {line}")
            else:
                print("âš ï¸ APIæ‰§è¡Œå‡ºç°è­¦å‘Šï¼Œä½†ç»§ç»­åˆ†æ...")
                print("é”™è¯¯è¾“å‡º:", result.stderr[-500:] if result.stderr else "æ— ")
                
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–å¤±è´¥: {str(e)}")
            print("ğŸ“Š å°†ä½¿ç”¨ç°æœ‰CSVæ–‡ä»¶è¿›è¡Œåˆ†æ...")
    
    # 4. ç¡®å®šCSVæ–‡ä»¶è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    csv_path = os.path.join(project_root, selected_item['csv'])
    
    if not os.path.exists(csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        print("ğŸ’¡ å»ºè®®å…ˆè¿è¡ŒAPIè·å–æ•°æ®")
        return None
    
    print(f"\nğŸ“Š å¼€å§‹åˆ†æ {selected_item['name']} æ•°æ®...")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {csv_path}")
    
    # 5. è¿›è¡Œæœºå™¨å­¦ä¹ åˆ†æ
    try:
        analyzer = MarketMLAnalyzer(csv_path)
        predictions = analyzer.run_complete_analysis(days_ahead=7)
        
        if predictions is not None:
            print(f"\nğŸ¯ {selected_item['name']} åˆ†æå®Œæˆ!")
            print("ğŸ“ˆ æœªæ¥7å¤©ä»·æ ¼é¢„æµ‹:")
            print(predictions)
            
            # 6. ç”Ÿæˆåˆ†ææ€»ç»“
            print(f"\nğŸ“‹ {selected_item['name']} åˆ†ææ€»ç»“:")
            print("=" * 50)
            
            # åŸºæœ¬ç»Ÿè®¡
            avg_price = predictions['predicted_price'].mean()
            price_trend = predictions['predicted_price'].iloc[-1] - predictions['predicted_price'].iloc[0]
            
            print(f"ğŸ“Š å¹³å‡é¢„æµ‹ä»·æ ¼: {avg_price:.2f}")
            print(f"ğŸ“ˆ 7å¤©ä»·æ ¼è¶‹åŠ¿: {price_trend:+.2f} ({price_trend/predictions['predicted_price'].iloc[0]*100:+.1f}%)")
            
            # æŠ•èµ„å»ºè®®
            if price_trend > 0:
                print("ğŸ’¡ æŠ•èµ„å»ºè®®: ğŸ“ˆ ä»·æ ¼å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œè€ƒè™‘ä¹°å…¥")
            elif price_trend < -1:
                print("ğŸ’¡ æŠ•èµ„å»ºè®®: ğŸ“‰ ä»·æ ¼å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œè€ƒè™‘å–å‡º")
            else:
                print("ğŸ’¡ æŠ•èµ„å»ºè®®: â¡ï¸  ä»·æ ¼ç›¸å¯¹ç¨³å®šï¼Œè§‚æœ›")
            
            # é£é™©è¯„ä¼°
            ci_width = (predictions['ci_upper'] - predictions['ci_lower']).mean()
            risk_level = "ä½é£é™©" if ci_width < 2 else "ä¸­é£é™©" if ci_width < 5 else "é«˜é£é™©"
            print(f"âš ï¸  é£é™©è¯„ä¼°: {risk_level} (ç½®ä¿¡åŒºé—´å®½åº¦: Â±{ci_width/2:.2f})")
            
            return predictions
        else:
            print("âŒ åˆ†æå¤±è´¥")
            return None
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    while True:
        print("\nğŸ¯ Darker Market æœºå™¨å­¦ä¹ åˆ†æç³»ç»Ÿ")
        print("=" * 60)
        print("1. ğŸš€ æ™ºèƒ½æ¨¡å¼ (é€‰æ‹©ç‰©å“ â†’ è·å–æ•°æ® â†’ è‡ªåŠ¨åˆ†æ)")
        print("2. ğŸ“Š ä¼ ç»Ÿæ¨¡å¼ (åˆ†æç°æœ‰CSVæ–‡ä»¶)")
        print("3. ğŸ› ï¸ ç‰©å“é…ç½®ç®¡ç†")
        print("4. âŒ é€€å‡º")
        
        try:
            choice = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1-4, é»˜è®¤1): ").strip()
            
            if choice == "2":
                main_traditional()
            elif choice == "3":
                manage_items_config()
                # é…ç½®ç®¡ç†åè¿”å›ä¸»èœå•
                continue
            elif choice == "4":
                print("ğŸ‘‹ å†è§!")
                break
            else:
                # é»˜è®¤æˆ–é€‰æ‹©1
                collect_and_analyze()
                
            # è¯¢é—®æ˜¯å¦ç»§ç»­
            continue_choice = input("\nğŸ”„ æ˜¯å¦ç»§ç»­ä½¿ç”¨ç³»ç»Ÿ? (y/n, é»˜è®¤n): ").lower().strip()
            if continue_choice not in ['y', 'yes', 'æ˜¯']:
                print("ğŸ‘‹ å†è§!")
                break
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
            break
        except Exception as e:
            print(f"âŒ ç¨‹åºå‡ºé”™: {str(e)}")
            continue


def main_traditional():
    """ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥é€‰æ‹©CSVæ–‡ä»¶è¿›è¡Œåˆ†æ"""
    print("\nğŸ“ ä¼ ç»Ÿæ¨¡å¼ï¼šé€‰æ‹©ç°æœ‰CSVæ–‡ä»¶")
    print("=" * 40)
    
    import os
    import glob
    
    # è·å–é¡¹ç›®æ ¹ç›®å½• (è„šæœ¬åœ¨Analysiså­ç›®å½•ä¸­)
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    # è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_pattern = os.path.join(project_root, "*.csv")
    csv_files = [os.path.basename(f) for f in glob.glob(csv_pattern)]
    
    # æŒ‰æ–‡ä»¶åæ’åº
    csv_files.sort()
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ï¼")
        print("ğŸ’¡ å»ºè®®å…ˆä½¿ç”¨æ™ºèƒ½æ¨¡å¼è·å–æ•°æ®")
        return None
    
    print("ğŸ“„ å¯ç”¨çš„æ•°æ®æ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        print(f"  {i}. {file}")
    
    try:
        choice = int(input("\nè¯·é€‰æ‹©æ•°æ®æ–‡ä»¶ (è¾“å…¥æ•°å­—): ")) - 1
        if 0 <= choice < len(csv_files):
            csv_file = csv_files[choice]
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶:", csv_files[0])
            csv_file = csv_files[0]
    except:
        print("âŒ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶:", csv_files[0])
        csv_file = csv_files[0]
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    csv_file_path = os.path.join(project_root, csv_file)
    
    print(f"\nğŸ“Š å¼€å§‹åˆ†ææ–‡ä»¶: {csv_file}")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MarketMLAnalyzer(csv_file_path)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    predictions = analyzer.run_complete_analysis(days_ahead=7)
    
    if predictions is not None:
        print(f"\nğŸ¯ é¢„æµ‹å®Œæˆ! æœªæ¥7å¤©çš„ä»·æ ¼é¢„æµ‹å·²ç”Ÿæˆ")
        print(predictions)
        return predictions
    else:
        print("âŒ åˆ†æå¤±è´¥")
        return None


if __name__ == "__main__":
    main()
