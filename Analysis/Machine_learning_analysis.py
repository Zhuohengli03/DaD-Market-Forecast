import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import VotingRegressor, BaggingRegressor
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MarketMLAnalyzer:
    def __init__(self, csv_file_path):
        """
        åˆå§‹åŒ–æœºå™¨å­¦ä¹ åˆ†æå™¨
        
        Args:
            csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        """
        self.csv_file_path = csv_file_path
        self.df = None
        self.X = None
        self.y = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.models = {}
        self.best_model = None
        self.feature_importance = None
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸ”„ åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®
            self.df = pd.read_csv(self.csv_file_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(self.df)} æ¡è®°å½•")
            
            # æ•°æ®åŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ“… æ—¶é—´èŒƒå›´: {self.df['created_at'].min()} åˆ° {self.df['created_at'].max()}")
            
            # æ•°æ®é¢„å¤„ç†
            self._preprocess_data()
            
            # ç‰¹å¾å·¥ç¨‹
            self._feature_engineering()
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            self._prepare_training_data()
            
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return False
            
        return True
    
    def _preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ”§ è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        
        # è½¬æ¢æ—¶é—´åˆ—
        self.df['created_at'] = pd.to_datetime(self.df['created_at'])
        self.df['insert_time'] = pd.to_datetime(self.df['insert_time'])
        
        # æ’åº
        self.df = self.df.sort_values('created_at').reset_index(drop=True)
        
        # å¤„ç†ç¼ºå¤±å€¼
        self.df = self.df.dropna()
        
        # æ•°æ®ç±»å‹è½¬æ¢
        self.df['quantity'] = pd.to_numeric(self.df['quantity'], errors='coerce')
        self.df['price_per_unit'] = pd.to_numeric(self.df['price_per_unit'], errors='coerce')
        self.df['price'] = pd.to_numeric(self.df['price'], errors='coerce')
        self.df['has_sold'] = self.df['has_sold'].astype(int)
        
        # ç§»é™¤å¼‚å¸¸å€¼
        self._remove_outliers()
        
        print(f"ğŸ“Š é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {self.df.shape}")
    
    def _remove_outliers(self):
        """ç§»é™¤å¼‚å¸¸å€¼ - ä½¿ç”¨æ”¹è¿›çš„Z-scoreæ–¹æ³•"""
        print("ğŸ” æ£€æµ‹å’Œç§»é™¤å¼‚å¸¸å€¼...")
        
        # ä½¿ç”¨Z-scoreæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼Œå¯¹ä»·æ ¼æ•°æ®ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
        numeric_columns = ['quantity', 'price_per_unit', 'price']
        original_shape = self.df.shape
        
        for col in numeric_columns:
            # è®¡ç®—Z-score
            z_scores = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())
            
            # å¯¹ä»·æ ¼æ•°æ®ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼ˆ2.5Ïƒï¼‰ï¼Œå¯¹æ•°é‡ä½¿ç”¨æ ‡å‡†é˜ˆå€¼ï¼ˆ3Ïƒï¼‰
            threshold = 2.5 if col in ['price_per_unit', 'price'] else 3.0
            outliers = z_scores > threshold
            outlier_count = outliers.sum()
            
            if outlier_count > 0:
                print(f"  {col}: å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼ (é˜ˆå€¼: {threshold}Ïƒ)")
                self.df = self.df[~outliers]
        
        removed_count = original_shape[0] - self.df.shape[0]
        print(f"ğŸ“Š ç§»é™¤å¼‚å¸¸å€¼åæ•°æ®å½¢çŠ¶: {self.df.shape} (ç§»é™¤äº† {removed_count} æ¡è®°å½•)")
    
    def _feature_engineering(self):
        """é«˜çº§ç‰¹å¾å·¥ç¨‹"""
        print("âš™ï¸ è¿›è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹...")
        
        # 1. åŸºç¡€æ—¶é—´ç‰¹å¾
        self.df['hour'] = self.df['created_at'].dt.hour
        self.df['day_of_week'] = self.df['created_at'].dt.dayofweek
        self.df['day_of_month'] = self.df['created_at'].dt.day
        self.df['month'] = self.df['created_at'].dt.month
        self.df['is_weekend'] = (self.df['day_of_week'] >= 5).astype(int)
        
        # 2. ä»·æ ¼ç›¸å…³ç‰¹å¾
        self.df['price_quantity_ratio'] = self.df['price'] / (self.df['quantity'] + 1)
        self.df['price_per_unit_squared'] = self.df['price_per_unit'] ** 2
        self.df['quantity_squared'] = self.df['quantity'] ** 2
        self.df['price_quantity_interaction'] = self.df['price_per_unit'] * self.df['quantity']
        
        # 3. ç§»åŠ¨å¹³å‡ç‰¹å¾ (å¤šæ—¶é—´çª—å£)
        for window in [3, 5, 7, 10, 14]:
            self.df[f'price_ma_{window}'] = self.df['price_per_unit'].rolling(window=window).mean()
            self.df[f'quantity_ma_{window}'] = self.df['quantity'].rolling(window=window).mean()
            self.df[f'price_ema_{window}'] = self.df['price_per_unit'].ewm(span=window).mean()
        
        # 4. ä»·æ ¼å˜åŒ–ç‰¹å¾
        self.df['price_change'] = self.df['price_per_unit'].diff()
        self.df['price_change_pct'] = self.df['price_per_unit'].pct_change()
        self.df['price_change_abs'] = np.abs(self.df['price_change'])
        
        # 5. æ³¢åŠ¨æ€§ç‰¹å¾
        for window in [3, 5, 10, 20]:
            self.df[f'price_volatility_{window}'] = self.df['price_per_unit'].rolling(window=window).std()
            self.df[f'price_skewness_{window}'] = self.df['price_per_unit'].rolling(window=window).skew()
            self.df[f'price_kurtosis_{window}'] = self.df['price_per_unit'].rolling(window=window).kurt()
        
        # 6. è¶‹åŠ¿ç‰¹å¾
        for window in [5, 10, 20]:
            self.df[f'price_trend_{window}'] = self.df['price_per_unit'].rolling(window=window).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == window else np.nan
            )
        
        # 7. åˆ†ä½æ•°ç‰¹å¾
        for window in [10, 20]:
            self.df[f'price_q25_{window}'] = self.df['price_per_unit'].rolling(window=window).quantile(0.25)
            self.df[f'price_q75_{window}'] = self.df['price_per_unit'].rolling(window=window).quantile(0.75)
            self.df[f'price_iqr_{window}'] = self.df[f'price_q75_{window}'] - self.df[f'price_q25_{window}']
        
        # 8. æ»åç‰¹å¾
        for lag in [1, 2, 3, 5]:
            self.df[f'price_lag_{lag}'] = self.df['price_per_unit'].shift(lag)
            self.df[f'quantity_lag_{lag}'] = self.df['quantity'].shift(lag)
        
        # 9. äº¤äº’ç‰¹å¾
        self.df['hour_quantity_interaction'] = self.df['hour'] * self.df['quantity']
        self.df['day_quantity_interaction'] = self.df['day_of_week'] * self.df['quantity']
        
        # 10. å¡«å……ç¼ºå¤±å€¼
        self.df = self.df.fillna(method='ffill').fillna(method='bfill')
        
        print("âœ… é«˜çº§ç‰¹å¾å·¥ç¨‹å®Œæˆ")
    
    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # é€‰æ‹©ä¼˜åŒ–ç‰¹å¾ - é¿å…æ•°æ®æ³„éœ²ï¼Œå¢åŠ é¢„æµ‹èƒ½åŠ›
        feature_columns = [
            # åŸºç¡€ç‰¹å¾
            'quantity', 'has_sold',
            # æ—¶é—´ç‰¹å¾
            'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend',
            # ä»·æ ¼ç›¸å…³ç‰¹å¾ï¼ˆä¸åŒ…å«å½“å‰ä»·æ ¼ï¼‰
            'price_quantity_ratio', 'quantity_squared', 'price_quantity_interaction',
            # æ•°é‡ç§»åŠ¨å¹³å‡
            'quantity_ma_3', 'quantity_ma_5', 'quantity_ma_7', 'quantity_ma_10', 'quantity_ma_14',
            # æ»åç‰¹å¾
            'quantity_lag_1', 'quantity_lag_2', 'quantity_lag_3',
            # æ³¢åŠ¨æ€§ç‰¹å¾
            'price_volatility_3', 'price_volatility_5', 'price_volatility_10', 'price_volatility_20',
            # è¶‹åŠ¿ç‰¹å¾
            'price_trend_5', 'price_trend_10', 'price_trend_20',
            # åˆ†ä½æ•°ç‰¹å¾
            'price_q25_10', 'price_q75_10', 'price_iqr_10',
            'price_q25_20', 'price_q75_20', 'price_iqr_20',
            # äº¤äº’ç‰¹å¾
            'hour_quantity_interaction', 'day_quantity_interaction'
        ]
        
        # ç§»é™¤åŒ…å«NaNçš„åˆ—
        available_features = [col for col in feature_columns if col in self.df.columns]
        self.X = self.df[available_features].fillna(0)
        self.y = self.df['price_per_unit']
        
        print(f"ğŸ“Š ç‰¹å¾æ•°é‡: {len(available_features)}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(self.X)}")
        
        # åˆ†å‰²æ•°æ® - ä½¿ç”¨å›ºå®šæ¯”ä¾‹ç¡®ä¿ç¨³å®šæ€§
        split_index = int(len(self.X) * 0.8)
        
        # ç¡®ä¿åˆ†å‰²ç‚¹å›ºå®šï¼Œé¿å…éšæœºæ€§
        self.X_train = self.X.iloc[:split_index].copy()
        self.X_test = self.X.iloc[split_index:].copy()
        self.y_train = self.y.iloc[:split_index].copy()
        self.y_test = self.y.iloc[split_index:].copy()
        
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(self.X_train)}, æµ‹è¯•é›†å¤§å°: {len(self.X_test)}")
        print(f"ğŸ“Š åˆ†å‰²ç‚¹: ç¬¬{split_index}æ¡è®°å½• ({(split_index/len(self.X)*100):.1f}%)")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print("âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
    
    def train_models(self):
        """è®­ç»ƒå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # å®šä¹‰ä¼˜åŒ–æ¨¡å‹ - å¢å¼ºæ€§èƒ½å’Œç¨³å®šæ€§
        models = {
            # é›†æˆå­¦ä¹ æ¨¡å‹
            'Random Forest': RandomForestRegressor(
                n_estimators=200, max_depth=12, min_samples_split=5, 
                min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1
            ),
            'Extra Trees': ExtraTreesRegressor(
                n_estimators=200, max_depth=12, min_samples_split=5,
                min_samples_leaf=2, max_features='sqrt', random_state=42, n_jobs=-1
            ),
            'Gradient Boosting': GradientBoostingRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.03, 
                subsample=0.8, random_state=42
            ),
            'XGBoost': XGBRegressor(
                n_estimators=200, max_depth=6, learning_rate=0.03,
                subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
            ),
            # çº¿æ€§æ¨¡å‹
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=0.1, random_state=42),
            'Lasso Regression': Lasso(alpha=0.01, random_state=42, max_iter=5000),
            'Elastic Net': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42, max_iter=5000),
            # æ”¯æŒå‘é‡æœº
            'SVR': SVR(kernel='rbf', C=10.0, gamma='auto', epsilon=0.01),
            # ç¥ç»ç½‘ç»œ
            'MLP Regressor': MLPRegressor(
                hidden_layer_sizes=(100, 50), activation='relu', solver='adam',
                alpha=0.001, learning_rate='adaptive', max_iter=1000, random_state=42
            )
        }
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        model_scores = {}
        
        for name, model in models.items():
            print(f"ğŸ”„ è®­ç»ƒ {name}...")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                if name == 'SVR':
                    model.fit(self.X_train_scaled, self.y_train)
                    y_pred = model.predict(self.X_test_scaled)
                else:
                    model.fit(self.X_train, self.y_train)
                    y_pred = model.predict(self.X_test)
                
                # è¯„ä¼°æ¨¡å‹
                mae = mean_absolute_error(self.y_test, y_pred)
                mse = mean_squared_error(self.y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(self.y_test, y_pred)
                
                # æ·»åŠ äº¤å‰éªŒè¯è¯„ä¼°
                if name == 'SVR':
                    cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=3, scoring='r2')
                else:
                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3, scoring='r2')
                
                model_scores[name] = {
                    'model': model,
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'cv_r2_mean': cv_scores.mean(),
                    'cv_r2_std': cv_scores.std(),
                    'predictions': y_pred
                }
                
                print(f"  âœ… {name}: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.3f}, CV-RÂ²={cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
                
            except Exception as e:
                print(f"  âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
        
        self.models = model_scores
        
        # åˆ›å»ºé›†æˆæ¨¡å‹
        print("\nğŸ”„ åˆ›å»ºé›†æˆæ¨¡å‹...")
        ensemble_models = self._create_ensemble_model(model_scores)
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        if ensemble_models:
            ensemble_scores = self._evaluate_ensemble_model(ensemble_models)
            model_scores['Ensemble'] = ensemble_scores
            print(f"âœ… é›†æˆæ¨¡å‹: RÂ²={ensemble_scores['r2']:.3f}, CV-RÂ²={ensemble_scores['cv_r2_mean']:.3f}")
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ - ä½¿ç”¨äº¤å‰éªŒè¯åˆ†æ•°
        best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['cv_r2_mean'])
        self.best_model = model_scores[best_model_name]['model']
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"ğŸ“Š æœ€ä½³æ¨¡å‹æ€§èƒ½: RÂ²={model_scores[best_model_name]['r2']:.3f}, CV-RÂ²={model_scores[best_model_name]['cv_r2_mean']:.3f}")
        
        # è¯¦ç»†æ¨¡å‹å‡†ç¡®æ€§åˆ†æ
        self._analyze_model_accuracy(model_scores[best_model_name], best_model_name)
        
        return model_scores
    
    def _create_ensemble_model(self, model_scores):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        try:
            # é€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹è¿›è¡Œé›†æˆ
            top_models = sorted(model_scores.items(), key=lambda x: x[1]['cv_r2_mean'], reverse=True)[:5]
            
            if len(top_models) < 2:
                print("âš ï¸ æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œè·³è¿‡é›†æˆ")
                return None
            
            # åˆ›å»ºæŠ•ç¥¨å›å½’å™¨
            estimators = []
            for name, scores in top_models:
                if name in ['SVR', 'MLP Regressor']:
                    # éœ€è¦æ ‡å‡†åŒ–çš„æ¨¡å‹
                    estimators.append((name, scores['model']))
                else:
                    estimators.append((name, scores['model']))
            
            # ä½¿ç”¨åŠ æƒå¹³å‡
            ensemble = VotingRegressor(estimators, weights=[scores[1]['cv_r2_mean'] for scores in top_models])
            
            return {
                'voting': ensemble,
                'models': top_models,
                'weights': [scores[1]['cv_r2_mean'] for scores in top_models]
            }
            
        except Exception as e:
            print(f"âš ï¸ é›†æˆæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
            return None
    
    def _evaluate_ensemble_model(self, ensemble_models):
        """è¯„ä¼°é›†æˆæ¨¡å‹"""
        try:
            ensemble = ensemble_models['voting']
            
            # è®­ç»ƒé›†æˆæ¨¡å‹
            ensemble.fit(self.X_train, self.y_train)
            
            # é¢„æµ‹
            y_pred = ensemble.predict(self.X_test)
            
            # è¯„ä¼°
            mae = mean_absolute_error(self.y_test, y_pred)
            mse = mean_squared_error(self.y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(self.y_test, y_pred)
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(ensemble, self.X_train, self.y_train, cv=3, scoring='r2')
            
            return {
                'model': ensemble,
                'mae': mae,
                'mse': mse,
                'rmse': rmse,
                'r2': r2,
                'cv_r2_mean': cv_scores.mean(),
                'cv_r2_std': cv_scores.std(),
                'predictions': y_pred
            }
            
        except Exception as e:
            print(f"âš ï¸ é›†æˆæ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def _analyze_model_accuracy(self, model_info, model_name):
        """è¯¦ç»†åˆ†ææ¨¡å‹å‡†ç¡®æ€§"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹å‡†ç¡®æ€§è¯¦ç»†åˆ†æ")
        print("="*60)
        
        # 1. åŸºç¡€æŒ‡æ ‡åˆ†æ
        print("\nğŸ” åŸºç¡€è¯„ä¼°æŒ‡æ ‡:")
        print(f"  â€¢ RÂ² å†³å®šç³»æ•°: {model_info['r2']:.4f} (è¶Šæ¥è¿‘1è¶Šå¥½)")
        print(f"  â€¢ äº¤å‰éªŒè¯ RÂ²: {model_info['cv_r2_mean']:.4f} Â± {model_info['cv_r2_std']:.4f}")
        print(f"  â€¢ å¹³å‡ç»å¯¹è¯¯å·® (MAE): {model_info['mae']:.4f}")
        print(f"  â€¢ å‡æ–¹æ ¹è¯¯å·® (RMSE): {model_info['rmse']:.4f}")
        
        # 2. å‡†ç¡®æ€§ç­‰çº§è¯„ä¼°
        r2_score = model_info['r2']
        cv_r2_score = model_info['cv_r2_mean']
        
        print("\nğŸ“ˆ å‡†ç¡®æ€§ç­‰çº§è¯„ä¼°:")
        if r2_score >= 0.95:
            print("  âœ… ä¼˜ç§€ (RÂ² â‰¥ 0.95) - æ¨¡å‹é¢„æµ‹éå¸¸å‡†ç¡®")
        elif r2_score >= 0.85:
            print("  âœ… è‰¯å¥½ (0.85 â‰¤ RÂ² < 0.95) - æ¨¡å‹é¢„æµ‹è¾ƒä¸ºå‡†ç¡®")
        elif r2_score >= 0.70:
            print("  âš ï¸  ä¸€èˆ¬ (0.70 â‰¤ RÂ² < 0.85) - æ¨¡å‹é¢„æµ‹åŸºæœ¬å¯ç”¨")
        elif r2_score >= 0.50:
            print("  âš ï¸  è¾ƒå·® (0.50 â‰¤ RÂ² < 0.70) - æ¨¡å‹é¢„æµ‹ä¸å¤Ÿå‡†ç¡®")
        else:
            print("  âŒ å¾ˆå·® (RÂ² < 0.50) - æ¨¡å‹é¢„æµ‹ä¸å‡†ç¡®")
        
        # 3. è¿‡æ‹Ÿåˆæ£€æµ‹
        print("\nğŸ” è¿‡æ‹Ÿåˆæ£€æµ‹:")
        r2_diff = r2_score - cv_r2_score
        if r2_diff > 0.1:
            print(f"  âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        elif r2_diff > 0.05:
            print(f"  âš ï¸  è½»å¾®è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        else:
            print(f"  âœ… æ— è¿‡æ‹Ÿåˆ (RÂ²å·®å¼‚: {r2_diff:.3f})")
        
        # 4. é¢„æµ‹è¯¯å·®åˆ†æ
        y_pred = model_info['predictions']
        errors = self.y_test - y_pred
        
        print("\nğŸ“Š é¢„æµ‹è¯¯å·®åˆ†æ:")
        print(f"  â€¢ å¹³å‡è¯¯å·®: {errors.mean():.4f} (è¶Šæ¥è¿‘0è¶Šå¥½)")
        print(f"  â€¢ è¯¯å·®æ ‡å‡†å·®: {errors.std():.4f}")
        print(f"  â€¢ æœ€å¤§æ­£è¯¯å·®: {errors.max():.4f}")
        print(f"  â€¢ æœ€å¤§è´Ÿè¯¯å·®: {errors.min():.4f}")
        
        # 5. ç›¸å¯¹è¯¯å·®åˆ†æ
        relative_errors = np.abs(errors) / self.y_test * 100
        print(f"  â€¢ å¹³å‡ç›¸å¯¹è¯¯å·®: {relative_errors.mean():.2f}% (è¶Šæ¥è¿‘0è¶Šå¥½)")
        print(f"  â€¢ ç›¸å¯¹è¯¯å·®ä¸­ä½æ•°: {relative_errors.median():.2f}%")
        
        # 6. é¢„æµ‹èŒƒå›´åˆ†æ
        print("\nğŸ“ˆ é¢„æµ‹èŒƒå›´åˆ†æ:")
        print(f"  â€¢ å®é™…ä»·æ ¼èŒƒå›´: {self.y_test.min():.2f} - {self.y_test.max():.2f}")
        print(f"  â€¢ é¢„æµ‹ä»·æ ¼èŒƒå›´: {y_pred.min():.2f} - {y_pred.max():.2f}")
        print(f"  â€¢ ä»·æ ¼èŒƒå›´è¦†ç›–ç‡: {((y_pred >= self.y_test.min()) & (y_pred <= self.y_test.max())).mean()*100:.1f}%")
        
        # 7. æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°
        print("\nğŸ›¡ï¸ æ¨¡å‹ç¨³å®šæ€§è¯„ä¼°:")
        cv_std = model_info['cv_r2_std']
        if cv_std < 0.01:
            print("  âœ… éå¸¸ç¨³å®š (CVæ ‡å‡†å·® < 0.01)")
        elif cv_std < 0.05:
            print("  âœ… ç¨³å®š (CVæ ‡å‡†å·® < 0.05)")
        elif cv_std < 0.10:
            print("  âš ï¸  ä¸€èˆ¬ç¨³å®š (CVæ ‡å‡†å·® < 0.10)")
        else:
            print("  âŒ ä¸ç¨³å®š (CVæ ‡å‡†å·® â‰¥ 0.10)")
        
        # 8. ä¸šåŠ¡ä»·å€¼è¯„ä¼°
        print("\nğŸ’¼ ä¸šåŠ¡ä»·å€¼è¯„ä¼°:")
        mae_percentage = (model_info['mae'] / self.y_test.mean()) * 100
        if mae_percentage < 5:
            print("  âœ… é«˜ä»·å€¼ - é¢„æµ‹è¯¯å·®å°äº5%ï¼Œé€‚åˆæŠ•èµ„å†³ç­–")
        elif mae_percentage < 10:
            print("  âœ… ä¸­é«˜ä»·å€¼ - é¢„æµ‹è¯¯å·®5-10%ï¼Œé€‚åˆè¶‹åŠ¿åˆ†æ")
        elif mae_percentage < 20:
            print("  âš ï¸  ä¸­ç­‰ä»·å€¼ - é¢„æµ‹è¯¯å·®10-20%ï¼Œé€‚åˆå‚è€ƒ")
        else:
            print("  âŒ ä½ä»·å€¼ - é¢„æµ‹è¯¯å·®å¤§äº20%ï¼Œä¸å»ºè®®ç”¨äºå†³ç­–")
        
        print("\n" + "="*60)
    
    def hyperparameter_tuning(self):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("âš™ï¸ å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ ¹æ®æœ€ä½³æ¨¡å‹ç±»å‹è¿›è¡Œè°ƒä¼˜
        if isinstance(self.best_model, RandomForestRegressor):
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            }
            model = RandomForestRegressor(random_state=42)
            
        elif isinstance(self.best_model, GradientBoostingRegressor):
            param_grid = {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            }
            model = GradientBoostingRegressor(random_state=42)
            
        else:
            print("âš ï¸ å½“å‰æœ€ä½³æ¨¡å‹ä¸æ”¯æŒè¶…å‚æ•°è°ƒä¼˜")
            return self.best_model
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            model, param_grid, cv=5, scoring='r2', n_jobs=-1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"ğŸ“Š æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.3f}")
        
        self.best_model = grid_search.best_estimator_
        return self.best_model
    
    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print("ğŸ” åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance = pd.DataFrame({
                'feature': self.X.columns,
                'importance': self.best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
            print(self.feature_importance.head(10))
            
        else:
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        
        return self.feature_importance
    
    def test_model_stability(self, n_runs=5):
        """æµ‹è¯•æ¨¡å‹ç¨³å®šæ€§"""
        print(f"\nğŸ”„ æµ‹è¯•æ¨¡å‹ç¨³å®šæ€§ ({n_runs} æ¬¡è¿è¡Œ)...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # å­˜å‚¨å¤šæ¬¡è¿è¡Œçš„ç»“æœ
        r2_scores = []
        mae_scores = []
        predictions_list = []
        
        for i in range(n_runs):
            print(f"  è¿è¡Œ {i+1}/{n_runs}...")
            
            # é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ï¼‰
            if hasattr(self.best_model, 'random_state'):
                self.best_model.random_state = 42 + i  # æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ç§å­
            
            # è®­ç»ƒæ¨¡å‹
            if hasattr(self.best_model, 'fit'):
                if 'SVR' in str(type(self.best_model)):
                    self.best_model.fit(self.X_train_scaled, self.y_train)
                    y_pred = self.best_model.predict(self.X_test_scaled)
                else:
                    self.best_model.fit(self.X_train, self.y_train)
                    y_pred = self.best_model.predict(self.X_test)
                
                # è¯„ä¼°
                r2 = r2_score(self.y_test, y_pred)
                mae = mean_absolute_error(self.y_test, y_pred)
                
                r2_scores.append(r2)
                mae_scores.append(mae)
                predictions_list.append(y_pred)
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        r2_mean = np.mean(r2_scores)
        r2_std = np.std(r2_scores)
        mae_mean = np.mean(mae_scores)
        mae_std = np.std(mae_scores)
        
        print(f"\nğŸ“Š ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
        print(f"  RÂ² å¹³å‡å€¼: {r2_mean:.4f} Â± {r2_std:.4f}")
        print(f"  MAE å¹³å‡å€¼: {mae_mean:.4f} Â± {mae_std:.4f}")
        
        # ç¨³å®šæ€§è¯„ä¼°
        if r2_std < 0.01:
            print("  âœ… éå¸¸ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.01)")
        elif r2_std < 0.05:
            print("  âœ… ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.05)")
        elif r2_std < 0.10:
            print("  âš ï¸  ä¸€èˆ¬ç¨³å®š (RÂ²æ ‡å‡†å·® < 0.10)")
        else:
            print("  âŒ ä¸ç¨³å®š (RÂ²æ ‡å‡†å·® â‰¥ 0.10)")
        
        return {
            'r2_scores': r2_scores,
            'mae_scores': mae_scores,
            'r2_mean': r2_mean,
            'r2_std': r2_std,
            'mae_mean': mae_mean,
            'mae_std': mae_std,
            'predictions': predictions_list
        }
    
    def predict_future_prices(self, days_ahead=7):
        """é¢„æµ‹æœªæ¥ä»·æ ¼ - æ”¹è¿›ç‰ˆæ—¶é—´åºåˆ—é¢„æµ‹"""
        print(f"ğŸ”® é¢„æµ‹æœªæ¥ {days_ahead} å¤©çš„ä»·æ ¼...")
        
        if self.best_model is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # ä½¿ç”¨æ—¶é—´åºåˆ—æ–¹æ³•è¿›è¡Œé¢„æµ‹
        predictions = []
        confidence_intervals = []
        
        # è·å–å†å²ä»·æ ¼æ•°æ®
        historical_prices = self.df['price_per_unit'].values
        historical_dates = self.df['created_at'].values
        
        # è®¡ç®—ä»·æ ¼è¶‹åŠ¿
        recent_prices = historical_prices[-30:]  # æœ€è¿‘30ä¸ªæ•°æ®ç‚¹
        if len(recent_prices) > 1:
            # è®¡ç®—è¶‹åŠ¿
            trend = np.polyfit(range(len(recent_prices)), recent_prices, 1)[0]
            volatility = np.std(recent_prices)
            mean_price = np.mean(recent_prices)
        else:
            trend = 0
            volatility = 0
            mean_price = historical_prices[-1]
        
        print(f"ğŸ“Š ä»·æ ¼è¶‹åŠ¿åˆ†æ:")
        print(f"  æœ€è¿‘ä»·æ ¼å‡å€¼: {mean_price:.2f}")
        print(f"  ä»·æ ¼è¶‹åŠ¿: {trend:+.4f} æ¯å•ä½æ—¶é—´")
        print(f"  ä»·æ ¼æ³¢åŠ¨æ€§: {volatility:.2f}")
        
        # ç”Ÿæˆé¢„æµ‹
        for day in range(days_ahead):
            # åŸºç¡€é¢„æµ‹ï¼šä½¿ç”¨è¶‹åŠ¿
            base_prediction = mean_price + trend * (day + 1)
            
            # æ·»åŠ éšæœºæ³¢åŠ¨ï¼ˆåŸºäºå†å²æ³¢åŠ¨æ€§ï¼‰
            if volatility > 0:
                noise = np.random.normal(0, volatility * 0.1)  # 10%çš„æ³¢åŠ¨
                prediction = base_prediction + noise
            else:
                prediction = base_prediction
            
            # ç¡®ä¿é¢„æµ‹ä»·æ ¼åˆç†ï¼ˆä¸èƒ½ä¸ºè´Ÿæ•°ï¼‰
            prediction = max(prediction, 0.01)
            
            predictions.append(prediction)
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            if volatility > 0:
                ci_lower = prediction - 1.96 * volatility * 0.1
                ci_upper = prediction + 1.96 * volatility * 0.1
            else:
                ci_lower = prediction * 0.95
                ci_upper = prediction * 1.05
            
            confidence_intervals.append((ci_lower, ci_upper))
        
        # åˆ›å»ºé¢„æµ‹ç»“æœ
        future_dates = pd.date_range(
            start=self.df['created_at'].max() + pd.Timedelta(days=1),
            periods=days_ahead,
            freq='D'
        )
        
        predictions_df = pd.DataFrame({
            'date': future_dates,
            'predicted_price': predictions,
            'ci_lower': [ci[0] for ci in confidence_intervals],
            'ci_upper': [ci[1] for ci in confidence_intervals]
        })
        
        print("âœ… ä»·æ ¼é¢„æµ‹å®Œæˆ")
        return predictions_df
    
    def plot_analysis(self):
        """ç»˜åˆ¶åˆ†æå›¾è¡¨"""
        print("ğŸ“Š ç»˜åˆ¶åˆ†æå›¾è¡¨...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. ä»·æ ¼æ—¶é—´åºåˆ—
        axes[0, 0].plot(self.df['created_at'], self.df['price_per_unit'], alpha=0.7)
        axes[0, 0].set_title('Price Time Series', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Date')
        axes[0, 0].set_ylabel('Price per Unit')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ä»·æ ¼åˆ†å¸ƒ
        axes[0, 1].hist(self.df['price_per_unit'], bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_title('Price Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Price per Unit')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        if self.models:
            model_names = list(self.models.keys())
            r2_scores = [self.models[name]['r2'] for name in model_names]
            
            bars = axes[1, 0].bar(model_names, r2_scores, alpha=0.7)
            axes[1, 0].set_title('Model Performance (RÂ² Score)', fontsize=14, fontweight='bold')
            axes[1, 0].set_ylabel('RÂ² Score')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, r2_scores):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{score:.3f}', ha='center', va='bottom')
        
        # 4. ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            top_features = self.feature_importance.head(10)
            axes[1, 1].barh(range(len(top_features)), top_features['importance'])
            axes[1, 1].set_yticks(range(len(top_features)))
            axes[1, 1].set_yticklabels(top_features['feature'])
            axes[1, 1].set_title('Top 10 Feature Importance', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('Importance')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def plot_predictions(self, predictions_df):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœ - æ”¹è¿›ç‰ˆ"""
        print("ğŸ“ˆ ç»˜åˆ¶é¢„æµ‹ç»“æœ...")
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 12))
        
        # 1. å†å²ä»·æ ¼å’Œé¢„æµ‹ï¼ˆå¸¦ç½®ä¿¡åŒºé—´ï¼‰
        axes[0].plot(self.df['created_at'], self.df['price_per_unit'], 
                    label='Historical Price', alpha=0.7, linewidth=1, color='blue')
        
        # é¢„æµ‹ä»·æ ¼
        axes[0].plot(predictions_df['date'], predictions_df['predicted_price'], 
                    label='Predicted Price', color='red', linewidth=2, linestyle='-', marker='o')
        
        # ç½®ä¿¡åŒºé—´
        if 'ci_lower' in predictions_df.columns:
            axes[0].fill_between(predictions_df['date'], 
                               predictions_df['ci_lower'], 
                               predictions_df['ci_upper'], 
                               alpha=0.3, color='red', label='95% Confidence Interval')
        
        axes[0].set_title('Price History and Future Prediction with Confidence Interval', 
                         fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Date')
        axes[0].set_ylabel('Price per Unit')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        axes[0].tick_params(axis='x', rotation=45)
        
        # 2. é¢„æµ‹è¶‹åŠ¿åˆ†æï¼ˆæ›´è¯¦ç»†ï¼‰
        axes[1].plot(predictions_df['date'], predictions_df['predicted_price'], 
                    marker='o', linewidth=3, markersize=8, color='red', label='Predicted Price')
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(predictions_df) > 1:
            x_numeric = range(len(predictions_df))
            z = np.polyfit(x_numeric, predictions_df['predicted_price'], 1)
            trend_line = np.poly1d(z)
            axes[1].plot(predictions_df['date'], trend_line(x_numeric), 
                        '--', color='orange', linewidth=2, alpha=0.8, 
                        label=f'Trend: {"ä¸Šå‡" if z[0] > 0 else "ä¸‹é™"} ({z[0]:.2f}/day)')
        
        axes[1].set_title('Future Price Trend Analysis', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Date')
        axes[1].set_ylabel('Predicted Price')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].tick_params(axis='x', rotation=45)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        current_price = self.df['price_per_unit'].iloc[-1]
        future_price = predictions_df['predicted_price'].iloc[-1]
        price_change = future_price - current_price
        price_change_pct = (price_change / current_price) * 100
        
        # è®¡ç®—ä»·æ ¼èŒƒå›´
        min_pred = predictions_df['predicted_price'].min()
        max_pred = predictions_df['predicted_price'].max()
        price_range = max_pred - min_pred
        
        stats_text = f'''Current: {current_price:.2f}
Future: {future_price:.2f}
Change: {price_change:+.2f} ({price_change_pct:+.1f}%)
Range: {min_pred:.2f} - {max_pred:.2f}
Volatility: {price_range:.2f}'''
        
        axes[1].text(0.02, 0.98, stats_text, transform=axes[1].transAxes, 
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        # æ‰“å°è¯¦ç»†é¢„æµ‹ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†é¢„æµ‹ç»“æœ:")
        print(f"å½“å‰ä»·æ ¼: {current_price:.2f}")
        print(f"é¢„æµ‹ä»·æ ¼èŒƒå›´: {min_pred:.2f} - {max_pred:.2f}")
        print(f"æœ€ç»ˆé¢„æµ‹ä»·æ ¼: {future_price:.2f}")
        print(f"ä»·æ ¼å˜åŒ–: {price_change:+.2f} ({price_change_pct:+.1f}%)")
        print(f"é¢„æµ‹æ³¢åŠ¨æ€§: {price_range:.2f}")
        
        # æ˜¾ç¤ºæ¯æ—¥é¢„æµ‹
        print(f"\nğŸ“… æ¯æ—¥é¢„æµ‹è¯¦æƒ…:")
        for i, row in predictions_df.iterrows():
            print(f"  {row['date'].strftime('%Y-%m-%d')}: {row['predicted_price']:.2f}")
        
        return predictions_df
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""
        ========================================
        æœºå™¨å­¦ä¹ ä»·æ ¼é¢„æµ‹åˆ†ææŠ¥å‘Š
        ========================================
        
        æ•°æ®æ¦‚è§ˆ:
        - æ€»è®°å½•æ•°: {len(self.df)}
        - æ—¶é—´èŒƒå›´: {self.df['created_at'].min()} åˆ° {self.df['created_at'].max()}
        - ç‰¹å¾æ•°é‡: {len(self.X.columns)}
        
        æ¨¡å‹æ€§èƒ½:
        """
        
        if self.models:
            for name, metrics in self.models.items():
                report += f"        - {name}: RÂ²={metrics['r2']:.3f}, RMSE={metrics['rmse']:.2f}\n"
        
        if self.feature_importance is not None:
            report += f"\n        é‡è¦ç‰¹å¾ (å‰5ä¸ª):\n"
            for i, row in self.feature_importance.head(5).iterrows():
                report += f"        - {row['feature']}: {row['importance']:.3f}\n"
        
        report += f"\n        æ•°æ®è´¨é‡:\n"
        report += f"        - ç¼ºå¤±å€¼: {self.df.isnull().sum().sum()}\n"
        report += f"        - å¼‚å¸¸å€¼: å·²å¤„ç†\n"
        report += f"        - æ•°æ®å®Œæ•´æ€§: {((len(self.df) - self.df.isnull().sum().sum()) / (len(self.df) * len(self.df.columns)) * 100):.1f}%\n"
        
        print(report)
        return report
    
    def run_complete_analysis(self, days_ahead=7):
        """è¿è¡Œå®Œæ•´çš„æœºå™¨å­¦ä¹ åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„æœºå™¨å­¦ä¹ åˆ†æ...")
        print("=" * 50)
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        if not self.load_and_prepare_data():
            return None
        
        # 2. è®­ç»ƒæ¨¡å‹
        model_scores = self.train_models()
        
        # 3. è¶…å‚æ•°è°ƒä¼˜
        self.hyperparameter_tuning()
        
        # 4. ç‰¹å¾é‡è¦æ€§åˆ†æ
        self.analyze_feature_importance()
        
        # 5. æ¨¡å‹ç¨³å®šæ€§æµ‹è¯•
        stability_results = self.test_model_stability(n_runs=3)
        
        # 6. é¢„æµ‹æœªæ¥ä»·æ ¼
        predictions = self.predict_future_prices(days_ahead)
        
        # 6. ç»˜åˆ¶åˆ†æå›¾è¡¨
        self.plot_analysis()
        
        # 7. ç»˜åˆ¶é¢„æµ‹ç»“æœ
        if predictions is not None:
            self.plot_predictions(predictions)
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        print("âœ… æœºå™¨å­¦ä¹ åˆ†æå®Œæˆ!")
        return predictions


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æœºå™¨å­¦ä¹ ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 50)
    
    # è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
    import os
    import glob
    
    # è·å–é¡¹ç›®æ ¹ç›®å½• (è„šæœ¬åœ¨Analysiså­ç›®å½•ä¸­)
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_pattern = os.path.join(project_root, "*.csv")
    csv_files = [os.path.basename(f) for f in glob.glob(csv_pattern)]
    
    # æŒ‰æ–‡ä»¶åæ’åº
    csv_files.sort()
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ï¼")
        return
    
    print("å¯ç”¨çš„æ•°æ®æ–‡ä»¶:")
    for i, file in enumerate(csv_files, 1):
        print(f"{i}. {file}")
    
    try:
        choice = int(input("è¯·é€‰æ‹©æ•°æ®æ–‡ä»¶ (è¾“å…¥æ•°å­—): ")) - 1
        if 0 <= choice < len(csv_files):
            csv_file = csv_files[choice]
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶:", csv_files[0])
            csv_file = csv_files[0]
    except:
        print("ä½¿ç”¨é»˜è®¤æ–‡ä»¶:", csv_files[0])
        csv_file = csv_files[0]
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    csv_file_path = os.path.join(project_root, csv_file)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MarketMLAnalyzer(csv_file_path)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    predictions = analyzer.run_complete_analysis(days_ahead=7)
    
    if predictions is not None:
        print(f"\nğŸ¯ é¢„æµ‹å®Œæˆ! æœªæ¥7å¤©çš„ä»·æ ¼é¢„æµ‹å·²ç”Ÿæˆ")
        print(predictions)


if __name__ == "__main__":
    main()
